<img width="958" height="631" alt="image" src="https://github.com/user-attachments/assets/b4d1fd16-359c-4df5-8fcd-6a25116535db" />


这个图是 **TGI持续批处理的工作流程示意图**，我用“食堂打饭”的类比给你讲明白~


### 先解释图里的关键“暗号”
- **Q（请求队列）**：像“排队打饭的人”，每个“人”（请求）有两个属性：
  - `input_tokens=1k`：相当于“你要的饭份量是1份”（输入文本的长度）；
  - `max_new_tokens=500`：相当于“你最多能加500勺菜”（要生成的回答最长长度）。
- **Router（路由器）**：像“食堂打饭的窗口阿姨”，负责安排谁先打饭、一次能接多少人。
- **MBP（max_batch_prefill_tokens=10k）**：阿姨一次最多能接10个人的“饭”（因为她的托盘一次只能装10份饭）。
- **TTB（total_token_budget=20.5k）**：食堂后厨一次最多能做20.5份“饭+菜”的总量（GPU显存上限）。
- **Bₚ（Prefill批次）**：阿姨一次接的10个人（刚好占满10k的MBP），这10个人先统一打“饭”（Prefill阶段，处理输入）。
- **LLM Engine（推理引擎）**：像“后厨”，负责把“饭+菜”做好（处理请求、生成回答）。


### 流程拆解（对应图的步骤）
1. **排队等打饭（Q队列）**：
   有10个人排队，每个人都是“1份饭+最多500勺菜”的需求。

2. **阿姨安排批次（Router）**：
   阿姨看自己的托盘最多装10份饭（MBP=10k），所以一次接10个人（刚好凑满10份饭），组成“Bₚ批次”。

3. **统一打饭（Prefill阶段）**：
   后厨给这10个人先把“饭”做好（Prefill处理输入），此时这10个人的“饭+菜”总量是：
   10份饭 + 10×500勺菜 = 15k（`batch_total_tokens=15k`），远小于后厨的最大产能20.5k（TTB）。

4. **边吃边加菜（Decode阶段）**：
   这10个人开始吃饭，同时后厨会**逐个给他们加菜**（Decode阶段逐一生成回答Token）。
   因为后厨还有剩余产能（20.5k-15k=5.5k），等这10个人加菜的过程中，阿姨还能接新的人来打饭（持续批处理的核心：不用等这10个人吃完，新的人可以插进来一起加菜）。


### 核心逻辑（为啥这么设计）
- 阿姨（Router）卡“一次10份饭”（MBP）：防止自己托盘装不下（GPU单次Prefill压力太大）；
- 后厨卡“20.5份总量”（TTB）：防止做太多饭/菜浪费（GPU显存溢出）；
- 边吃边加菜+插新人：不用等前一波人吃完，新的人可以一起加菜，让后厨一直忙（最大化GPU利用率）。


简单说：这个图就是讲“怎么让GPU在不撑死的前提下，一直高效地处理请求”~ 要不要我再用更直白的例子帮你串一遍？

这个图和文字是**TGI持续批处理中“显存预算不够时的动态调度逻辑”**，我结合“食堂打饭”的类比接着讲，保证你能懂~


### 先复习关键概念（对应食堂场景）
- **TTB（Total Token Budget=20.5k）**：后厨一次最多能做的“饭+菜”总量（GPU显存上限）；
- **请求的“饭+菜”= input_tokens（饭） + max_new_tokens（菜）**：每个请求的显存占用；
- **Router（阿姨）**：负责判断“新请求能不能塞进当前后厨的剩余产能”。


### 文字部分翻译+解读（场景描述）
文字说的是：
> 第13、14、15个请求“饭+菜”太多，超过了后厨剩余的产能（TTB不够），所以阿姨不让它们打饭（无法Prefill）；
> 但第16个请求“饭少”（input_tokens=500，比之前的1k小），“饭+菜”总量刚好能塞进剩余产能，所以阿姨让它打饭（成功Prefill），加入正在加菜的队伍（Decode批次）；
> 这时候后厨产能被塞满了，新请求只能等当前的人吃完（等正在运行的请求完成，释放产能）。


### 图的细节拆解（对应场景）
1. **左边Q队列（排队的人）**：
   - 第13、14、15个请求：都是“1k饭+500菜”，总占用=1500/人；
   - 第16个请求：是“500饭+500菜”，总占用=1000/人（饭少了一半）。

2. **Router（阿姨）的判断**：
   - 此时后厨剩余产能（TTB）只剩1k左右（图里`TTB=1k`）；
   - 第13/14/15个请求：1500>1k → 塞不下，不让进（`Exceeded TTB Limit`）；
   - 第16个请求：1000≤1k → 塞得下，让它进（加入`Bₚ`批次，做Prefill）。

3. **加入Decode批次（边吃边加菜）**：
   第16个请求打完饭（Prefill）后，直接加入正在“加菜”的队伍（`B_D`批次），和之前的请求一起逐次加菜；
   此时后厨产能被完全占满，新请求只能等有人吃完（请求完成，释放产能）才能进。


### 核心逻辑：“小请求插队，大请求等”
TGI的Router会**优先选“显存占用小的请求”塞进剩余预算**，这样能最大化利用GPU的剩余空间；而占用大的请求，只能等现有请求完成、释放显存后再处理——这就像食堂里，“要小碗饭的人能插队先打，要大碗饭的只能等前面的人吃完”~


要不要我再用更具体的数字（比如“剩余1k产能”怎么算的）帮你理一遍这个过程？

<img width="1070" height="656" alt="image" src="https://github.com/user-attachments/assets/5efc7eeb-d1fa-4b8b-9d6c-02aa9596bb11" />


单次前向传播的令牌预算，mbp，这十个请求耗尽了特定的干预填充阶段的mbp，

这段文字是在解释 **“为啥前10个请求没把显存用满，却不能加更多请求”**，我用“食堂打饭”的类比再讲透~


### 先抓核心问题
从之前的图能看到：前10个请求一起做了Prefill，但它们的总显存占用（15k）远小于TTB（20.5k）——那为啥不把更多请求加进来？


### 答案：被“MBP（Prefill阶段的单次上限）”卡住了
用食堂类比：
- **MBP（max_batch_prefill_tokens=10k）**：相当于“打饭阿姨的托盘一次最多能装10份饭”（Prefill阶段，GPU单次能处理的输入Token总量上限）；
- 前10个请求，每个的“饭”是1k → 10×1k=10k，刚好把阿姨的托盘装满（MBP用完了）；
- 哪怕后厨还有剩余产能（TTB没满），但阿姨的托盘装不下更多饭了，所以没法加新请求。


### 后续逻辑：Decode阶段可以补加请求
等这10个请求进入“加菜（Decode）”阶段后：
- 阿姨的托盘空出来了（Prefill的MBP限制只针对“打饭环节”）；
- 后厨还有剩余产能（TTB没满），所以阿姨可以接新请求（只要新请求的“饭+菜”能塞进剩余TTB），补到正在加菜的批次里——这就是“持续批处理”的灵活之处。


### 总结MBP和TTB的区别
| 预算类型 | 管啥阶段 | 作用 | 类比 |
|----------|----------|------|------|
| MBP      | Prefill（打饭） | 限制GPU单次能处理的输入Token总量 | 阿姨的托盘容量 |
| TTB      | 整个推理（饭+菜） | 限制GPU所有请求的总显存占用 | 后厨的总产能 |


简单说：**Prefill阶段是“先卡阿姨的托盘（MBP）”，Decode阶段是“再卡后厨的总产能（TTB）”**——这俩限制是分开的，所以才会出现“TTB没满但加不了请求”的情况。

要不要我再用具体数字（比如10个请求的MBP怎么算）帮你再理一遍？

这段文字是在讲 **LLM推理中“Prefill（预填充）”和“Decode（解码）”阶段的计算差异**，我用“写作文”的类比帮你理解：


### 先简化核心概念
- **前向传播（forward pass）**：模型“读输入、算输出”的过程（像你“读题目、写句子”）；
- **KV缓存**：模型把“之前算过的内容”存起来（像你写作文时“记住前面写的句子”）；
- **自回归**：解码时“后一句依赖前一句”（像你写作文“下一句得顺着上一句的意思写”）。


### 逐句拆解（结合类比）
#### 1. Prefill阶段（写作文的“审题+搭框架”）
> 预填充阶段，引擎对2000个Token做**完整的前向传播**，算出每个输入Token的“注意力查询/键/值”，最后生成每个序列的**第一个解码Token**。
> （类比：你写4篇作文，先把4篇的题目+开头（共2000字）全读一遍，理清每篇的思路，然后写出每篇的第一句话）


#### 2. Decode阶段（写作文的“顺着思路写后续内容”）
> 解码阶段，第N个Token会用**KV缓存**（之前算好的“查询/键/值”已经存好了），所以解码相当于**只对“第N个Token”做前向传播**。
> （类比：你写每篇作文的第二句时，不用再重读题目+开头，直接顺着第一句的思路写，只需要“想当前这一句”）


#### 3. 解码的“自回归+高效性”
> 解码是自回归的（逐Token生成），给4个序列生成2000个Token，相当于**同时只处理4个Token**（每个序列1个）；而Prefill阶段生成第一个Token时，得把2000个Token全过一遍模型。
> （类比：你给4篇作文各写500句（共2000句），每一步只需要“给每篇写1句”；但开头写第一句时，得把4篇的题目+开头全读一遍）


### 总结：Prefill“费劲儿”，Decode“省劲儿”
- **Prefill**：一次性处理所有输入Token，计算量大（相当于“把所有素材都过一遍”）；
- **Decode**：依赖KV缓存，每次只算1个新Token，计算量小（相当于“顺着之前的思路写”）。

这就是为什么Prefill阶段会有严格的Token限制（比如MBP），而Decode阶段能高效地持续加新请求~

要不要我用更具体的数字（比如2000Token的计算量差异）帮你再理一遍？

TGI开发人员做出了一个战略性选择，即在服务器层面使用Rust来实现连续批处理。在这种情况下，Rust的速度是最佳优势，因为Python在每个决策环节都会增加几毫秒的耗时。更准确地说，严格的类型检查和真正的并发能力使Rust相较于Python有了巨大提升。从规模角度考虑，对于单个请求批次，这种决策过程可能会发生100次，这将给端到端延迟增加数百毫秒。

warmup，kv caching ,flash and paged attention.

在总令牌预算估计方面，引擎，会将可用内存映射为可以处理的令牌的总数，

For the total token budget estimation, the engine maps available memory to a total count of processable tokens. First the engine calculates 95% of the available VRAM, leaving 5% room for error, where Available VRAM = GPU VRAM - Model VRAM - Prefill KV Cache VRAM. The available memory is then divided by the memory required to process a block of tokens [5] yielding the total number of tokens that can be processed simultaneously. This value is set as the MAX_BATCH_PREFILL_TOKENS, essentially the tokens that in a block times the number of blocks that fit into memory.
在总令牌预算估算方面，引擎会将可用内存映射为可处理令牌的总数。首先，引擎会计算可用显存的95%，留下5%作为误差空间，其中Available VRAM = GPU VRAM - Model VRAM - Prefill KV Cache VRAM。然后，将可用内存除以处理一个令牌块所需的内存[5]，得出可同时处理的令牌总数。该值被设为MAX_BATCH_PREFILL_TOKENS，本质上是一个块中的令牌数乘以内存可容纳的块数。

paged attention每一次内存易懂会影响延迟和吞吐量，为每一个请求重新创建键值缓存jv-cache张量会非常抵消，分页主力将键值缓存



flash attention，支持无填充张量的使用
