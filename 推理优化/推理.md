https://www.digitalocean.com/community/tutorials/llm-inference-optimization?utm_source=chatgpt.com

# What is LLM inference? 什么是大语言模型推理？


Similar to how one uses what they learned to solve a new problem, inference is when a trained AI model uses patterns detected during training to infer and make predictions on new data. This inference process is what enables LLMs to perform tasks like text completion, translation, summarization, and conversation.


# The Two Phases of LLM Inference LLM推理的两个阶段

 The prefill phase can be likened to reading an entire document at once and processing all the words simultaneously to write the first word whereas the decode phase can be cpmpared to continuing to write thi response word by word,where the choice of each word depends on what was written before.

 预填充阶段可以比作一次性阅读整篇文档，并同时处理所有词汇以写出第一个词，而解码阶段则可以比作逐词继续撰写这一回应，其中每个词的选择都取决于之前所写的内容。

 LLM inference can be divided into two phases: prefill and decode. These stages are separated due to the different computational requirements of each stage. While prefill, a highly-parallelized matrix-matrix operation that saturates GPU utilization, is compute-bound, decode, a matrix-vector operation that underutilizes the GPU compute capability, is memory-bound.

 预填充是高度并行化的矩阵-矩阵运算，能让gpu的利用率达到饱和，受到计算能力的限制，解码阶段是矩阵-向量运算，未能充分利用gpu的计算能力，受到内存的限制。


 <img width="1227" height="860" alt="image" src="https://github.com/user-attachments/assets/c6b22b7d-372e-4b3e-97b8-ff917623401b" />


预填充受到计算限制，解码受到内存限制

# prefill预填充

In the prefill stage, the LLM processes the entire input prompt at once to generate the first response token. This involves performing a full forward pass through the transformer layers for every token in the prompt simultaneously. While memory access is needed during prefill, the computational work of processing the tokens in parallel dominate the performance profile.

在预填充阶段，大语言模型会一次性处理整个输入提示词以生成第一个响应令牌。这包括同时对提示词中的每个令牌执行一次完整的通过Transformer层的前向传播。虽然预填充过程中需要进行内存访问，但并行处理令牌的计算工作在性能表现中占主导地位

# decode

In the decode stage, text is generated autoregressively where the next token is predicted one at time given all previous tokens. The decoding process is memory-bound due to its need to repeatedly access historical context. For each new token generated, the model must load the attention cache (key/value states, AKA KV cache) from all previous tokens, requiring frequent memory accesses that become more intensive as the sequence grows longer.

在解码阶段，文本以自回归的方式生成，即根据所有先前的令牌，每次预测下一个令牌。解码过程受内存限制，因为它需要反复访问历史上下文。对于生成的每个新令牌，模型必须加载所有先前令牌的注意力缓存（键/值状态，也称为KV缓存），这需要频繁的内存访问，且随着序列变长，访问会变得更加密集。


提示词于超，首次令牌输出的时间就越长，因为注意力机制需要整个输入序列来计算键值缓存，推理优化旨在最小化首次令牌输出的时间

ITL：令牌间延迟，输出令牌时间

连续token之间的平均时间，ITL告诉我们解码token生成的速度，一致的ITL是理想的，表明了高效的内存管理，高高gpu内存带宽一i优化良好的注意力计算

### LLM 推理优化核心指标整理表
| **指标** | **英文全称** | **定义** | **关注价值** |
|----------|--------------|----------|--------------|
| 首 Token 生成时间 | Time-to-First-Token（TTFT） | 处理输入提示词并生成**第一个响应 Token**所需的总时间，直接对应推理的**预填充阶段**耗时 | 提示词越长，TTFT 通常越久（因注意力机制需处理完整输入序列生成 KV 缓存）<br>推理优化的核心目标之一是**最小化 TTFT**，提升用户交互的即时性 |
| 令牌间延迟 | Inter-token Latency（ITL） | 连续生成两个 Token 之间的平均时间，直接对应推理的**解码阶段**速度 | 稳定的 ITL 代表：<br>1. 内存管理高效<br>2. GPU 内存带宽利用率高<br>3. 注意力计算流程优化<br>是衡量模型生成流畅度的关键指标 |

# optimizing prefill and decode

推测解码：

投机解码使用一个更小，更快的模型同时生成多个令牌，然后使用更大的目标模型对其进行验证，由于生成的样本和通过简单解码生成的样本来自完全相同的概率分布，投机解码在加快推理速度的同时，还能保持相同的响应质量


# chunked prefills and decode-maximal batching

分块预填充，和最大解码批处理


SARATHI shows how chunked prefills can enable the division of large prefills into manageable chunks, which can then be batched with decode requests (decode-maximal batching) for more efficient processing.
SARATHI展示了分块预填充如何能够将大型预填充分割为可管理的块，然后这些块可以与解码请求进行批处理（解码最大化批处理），以实现更高效的处理。

# batching 批处理

批处理将推理请求组合在一起，更大的批处理大小对应更高的吞吐量，

gpu片上内存有限，批处理大小智能增加到一定的程度。

batch size 批次大小

the critical ratio

To achieve maximum utilization of the hardware, one can try to find the critical ratio where there’s a balance between two key limiting factors:
为了实现硬件的最大利用率，可以尝试找到临界比率，在这一比率下，两个关键限制因素达到平衡：

在内存和计算单元之间传输权重所需要的时间，受到内存带宽的限制，The time needed to transfer weights between memory and compute units (limited by memory bandwidth)

the time required for actual computational operations(limited by FLOPS)

实际计算操作所需要的时间

While these two times are equal, the batch size can be increased without incurring any performance penalty. Beyond this point, increasing batch size would create bottlenecks in either memory transfer or computation. To determine an optimal batch size, profiling is important.


虽然这两个时间相等，但可以在不造成任何性能损失的情况下增大批量大小。超过这一点后，增大批量大小会在内存传输或计算方面造成瓶颈。要确定最佳批量大小，性能分析很重要。

通过量化减少模型权重的内存占用，利用改进的架构和注意力变体减少kv缓存的内存占用，以及通过并行处理从多个gpu当中的集中的内存


When looking at how memory is allocated in the GPU during serving, the model weights remain fixed and the activations only utilize a fraction of the GPU’s memory resources compared to the KV cache. Therefore, freeing up space for the KV cache is critical. This can be achieved by reducing the model weight memory footprint through quantization, reducing the KV cache memory footprint with modified architectures and attention variants, as well as pooling memory from multiple GPUs with parallelism.
在服务过程中观察GPU中的内存分配方式时，模型权重保持固定，与KV缓存相比，激活值仅占用GPU内存资源的一小部分。因此，为KV缓存释放空间至关重要。这可以通过以下方式实现：通过量化减少模型权重的内存占用，利用改进的架构和注意力变体减少KV缓存的内存占用，以及通过并行处理从多个GPU中集中内存

# quantization量化

存储模型参数（权重，激活值和梯度）所需要的位数。以内存换取精度来降低推理延迟。


# attention and its variants注意力以及变体


查询：表示上下文或者问题

建：表示被关注的信息

值：表示正在检索的信息

Attention weights are computed by comparing queries with keys, and then used to weight values, producing the final output representation.
注意力权重通过将查询与键进行比较来计算，然后用于对值进行加权，从而生成最终的输出表示。

<img width="1096" height="726" alt="image" src="https://github.com/user-attachments/assets/2b880969-ce5c-4274-b70b-baf5106b6e6c" />


<img width="1086" height="422" alt="image" src="https://github.com/user-attachments/assets/f2d47f47-afe8-42d4-b8fb-689bf5c540cc" />


<img width="1075" height="615" alt="image" src="https://github.com/user-attachments/assets/92ec6a69-a4a6-4b92-823e-2ca202f55ae8" />



mqa减少了建值缓存的内存大小，为了更大的批量大小流出了空间

<img width="1043" height="784" alt="image" src="https://github.com/user-attachments/assets/1f72a455-ab06-4d9b-a86a-f984527068b0" />


键值头的数量是不止一个的，但是少于多头注意力mha当中的查询数量，

<img width="1055" height="486" alt="image" src="https://github.com/user-attachments/assets/25757bb1-d644-404b-beff-5a0234877e7b" />

滑动窗口注意力（swa）或者局部注意力，将注意力限制在一个序列上滑动的固定大小的窗口内，

虽然滑动窗口注意力在扩展到长输入失效不高，当交错使用滑动窗口注意力和全局注意力，并且相邻的全局注意力曾共享一个kv缓存（跨层注意力）的时候，长序列的速度和质量不会受到影响


# local attention VS GLOBAL aTTENTION

local attention fucusing on token windows,enablling faster inference especially for long sequences,


Local and global attention mechanisms differ in key aspects. Local attention uses less computation (O(n * w)) and memory by focusing on token windows, enabling faster inference especially for long sequences, but may miss long-range dependencies. Global attention, while computationally more expensive (O(n^2)) and memory-intensive due to processing all token pairs, is able to better capture full context and long-range dependencies at the cost of slower inference speed.
局部注意力机制和全局注意力机制在关键方面存在差异。局部注意力通过聚焦于标记窗口，使用较少的计算量（O(n * w)）和内存，尤其在长序列中能实现更快的推理，但可能会遗漏长距离依赖关系。全局注意力虽然由于需要处理所有标记对，计算成本更高（O(n²)）且内存消耗更大，但能够更好地捕捉完整上下文和长距离依赖关系，不过代价是推理速度较慢。

# paged attention 分页注意力



# Paged Attention 分页注意力
Inspired by virtual memory allocation, PagedAttention proposed a framework for optimizing KV cache that takes the variation of the number of tokens across requests into consideration.

受虚拟内存分配的启发，分页注意力（PagedAttention）提出了一个用于优化KV缓存的框架，该框架考虑了请求间令牌数量的变化。

# FlashAttention 


There are three variations of FlashAttention, with FlashAttention-3 being the latest release and optimized for Hopper GPUs. Each iteration of this algorithm takes a hardware-aware approach to make the attention computation as fast as possible. Past articles written on FlashAttention include: Designing Hardware-Aware Algorithms: FlashAttention and FlashAttention-2
FlashAttention有三个版本，其中FlashAttention-3是最新发布的版本，针对Hopper GPU进行了优化。该算法的每一次迭代都采用硬件感知方法，以尽可能加快注意力计算的速度。过去关于FlashAttention的文章包括：设计硬件感知算法：FlashAttention和FlashAttention-2




# Model Architectures: Dense Models vs. Mixture of Experts
模型架构：密集型模型与专家混合模型



Dense LLMs are the standard where all parameters are actively engaged during inference.
密集型大语言模型是所有参数在推理过程中都积极参与的标准模型。


Mixture of Experts (MoE) LLMs are composed of multiple specialized sub-networks with a routing mechanism. Because only relevant experts are activated for each input, improved parameter efficiency and faster inference than dense models is often observed.
混合专家（MoE）大型语言模型由多个专门的子网络和一个路由机制组成。由于每个输入只会激活相关的专家，因此与密集型模型相比，通常能提高参数效率并加快推理速度。



# 并行性parallelism

Larger models often require multiple GPUs to run effectively. There are a number of different parallelization strategies that allow for multi-GPU inference.
更大的模型通常需要多个GPU才能有效运行。有多种不同的并行化策略可用于多GPU推理。


数据并行，张量并行，流水线并行，模型垂直方向，将完整的模型流水线的不同阶段并行拆分，通过重叠不同模型的阶段的计算来提高吞吐量

上下文，输入序列，将输入序列跨设备分割成段，减少长序列输入的内存的瓶颈

moe，专家

完全分片数据fully shared data，数据模型，优化器和梯度，在设备建分片组件，并行处理数据，并且在每个训练步骤后进行同步，参数会根据需要从分片当中获取和重构，用于计算然后立即丢弃，减少内存的占用，

# LLM推理并行化技术分类表
| **并行类型** | **拆分对象（分区）** | **核心描述** | **核心用途** |
|--------------|----------------------|--------------|--------------|
| 数据并行（Data） | 数据批次（Data） | 在不同设备间拆分不同批次的数据 | 为单设备无法容纳的大型数据集分配内存与计算资源，提升数据处理效率 |
| 张量并行（Tensor） | 权重张量（Weight Tensors） | 在多个设备上按行或按列拆分模型权重张量 | 为单设备无法容纳的大型张量分配内存与计算资源，支持大模型的推理部署 |
| 流水线并行（Pipeline） | 模型层（垂直方向）（Model Layers (vertically)） | 将完整模型的流水线拆分为不同阶段，并行执行各阶段计算 | 通过重叠不同模型阶段的计算过程，提升整体推理吞吐量 |
| 上下文并行（Context） | 输入序列（Input Sequences） | 将长输入序列跨设备分割为多个片段，分别处理 | 减少长序列输入时的内存瓶颈，提升长文本推理的效率 |
| 专家并行（Expert） | 混合专家模型的专家子网络（MoE models） | 在多个设备间拆分MoE模型的专家子网络，每个设备承载部分专家 | 通过分布式计算扩展专家数量，支持更大规模的MoE模型，同时提升推理性能 |
| 完全分片数据并行（Fully Sharded Data） | 数据、模型、优化器、梯度（Data, model, optimizer, and gradients） | 在设备间分片所有核心组件，并行处理数据；训练步后同步参数，按需从分片加载、重构参数，计算后立即释放 | 支持训练/推理远超单设备内存容量的超大规模模型，大幅降低单设备的内存占用 |

我可以帮你把这份表格和之前的**注意力变体表**整合为一份**LLM推理优化技术总览表**，方便你统一查阅，需要吗？

# 《LLM推理优化入门（LLM Inference Optimization 101）》网页总结
## 一、网页基础信息
- **发布主体**：DigitalOcean（云计算服务提供商）
- **发布时间**：2025年1月17日
- **作者**：Melani Maheswaran（DigitalOcean技术作家，拥有教学、数据质量、咨询及写作经验，毕业于皇后大学并获学士与硕士学位）
- **核心定位**：面向深度学习从业者与研究者，系统介绍LLM推理优化的核心概念、技术与指标，兼具理论性与实用性

## 二、LLM推理核心基础
### 1. 什么是LLM推理
类比人类运用已有知识解决新问题，训练完成的LLM通过训练阶段识别的模式，对新数据进行推断与预测，进而实现文本补全、翻译、总结、对话等任务。

### 2. 推理的两个关键阶段
| 阶段 | 核心任务 | 计算特点 | 性能瓶颈 |
|------|----------|----------|----------|
| 预填充（Prefill） | 一次性处理完整输入提示词，生成首个响应Token | 高度并行化的矩阵-矩阵运算，GPU利用率饱和 | 受计算能力限制（Compute-bound），需同时对提示词中所有Token执行Transformer层全前向传播 |
| 解码（Decode） | 自回归生成文本，基于历史Token逐一生成下一个Token | 矩阵-向量运算，GPU计算能力未充分利用 | 受内存限制（Memory-bound），需反复访问历史上下文的KV缓存，序列越长内存访问越密集 |

## 三、LLM推理性能评估指标
| 指标名称 | 英文全称 | 定义 | 关注价值 |
|----------|----------|------|----------|
| 首Token生成时间 | Time-to-First-Token（TTFT） | 处理提示词并生成首个响应Token的总时间，直接反映预填充阶段耗时 | 提示词越长，TTFT越久（注意力机制需完整输入序列计算KV缓存），优化核心目标是最小化TTFT以提升交互即时性 |
| 令牌间延迟 | Inter-token Latency（ITL） | 连续两个Token生成的平均时间，反映解码阶段速度 | 稳定的ITL代表内存管理高效、GPU内存带宽利用率高、注意力计算流程优化，是衡量生成流畅度的关键 |

## 四、LLM推理优化核心技术
### 1. 预填充与解码优化
- **投机解码（Speculative Decoding）**：用小型快速模型同步生成多个Token，再通过大型目标模型验证，在保持响应质量不变的前提下提升推理速度（生成样本与原生解码概率分布一致）。
- **分块预填充与解码最大化批处理（Chunked Prefills & Decode-Maximal Batching）**：参考SARATHI技术，将大型预填充任务拆分为可管理的块，与解码请求批量处理，提升整体效率。

### 2. 批处理（Batching）
- 核心逻辑：将多个推理请求分组，批大小越大通常吞吐量越高，但受GPU片上内存限制，批大小存在上限。
- 最优批大小选择：需找到“临界比率”，平衡内存与计算单元间权重传输时间（受内存带宽限制）、实际计算操作时间（受FLOPS限制），两者相等时批大小可增加且无性能损耗，需通过性能分析确定最优值。

### 3. KV缓存管理
KV缓存是GPU内存占用的主要部分（模型权重固定，激活值仅占少量内存），优化核心是释放缓存空间，主要方式包括：
- 通过量化减少模型权重内存占用；
- 用改进架构与注意力变体降低KV缓存内存 footprint；
- 借助并行技术整合多GPU内存。

### 4. 量化（Quantization）
减少模型参数（权重、激活值、梯度）的存储位数，以“内存换精度”的方式降低推理延迟，是平衡性能与资源消耗的关键技术。

### 5. 注意力机制及其变体
| 变体类型 | 核心改进 | 优势 | 局限 |
|----------|----------|------|------|
| 缩放点积注意力（SDPA） | Transformer架构核心组件，允许模型同时关注输入序列不同部分并加权 | 基础注意力机制，奠定LLM上下文理解能力 | 无明显局限，是其他变体的基础 |
| 多头注意力（MHA） | 多个SDPA头并行运算 | 捕捉输入序列不同维度的丰富关联 | 内存占用较高 |
| 多查询注意力（MQA） | 多个注意力头共享1个键值头 | 大幅减少KV缓存大小，支持更大批处理 | 解码速度快于MHA，但可能存在响应质量下降 |
| 分组查询注意力（GQA） | 改进MQA，将查询分组对应多个键值头（键值头数量介于MQA和MHA之间） | 平衡MQA（速度）与MHA（质量），实现与MHA相当的质量、接近MQA的速度 | 无显著局限，是当前主流选择之一 |
| 滑动窗口注意力（SWA） | 限制注意力仅作用于滑动的固定大小窗口（局部注意力） | 减少计算量（O(n*w)）与内存占用，长序列推理更快 | 单独使用时可能遗漏长程依赖，需与全局注意力交织（如Character AI的跨层注意力共享KV缓存） |

此外，还包括**分页注意力（Paged Attention）**（借鉴虚拟内存分配，考虑请求间Token数量差异优化KV缓存）、**FlashAttention**（硬件感知型算法，FlashAttention-3针对Hopper GPU优化，最大化注意力计算速度）。

### 6. 模型架构优化：稠密模型vs专家混合模型
| 架构类型 | 特点 | 性能表现 |
|----------|------|----------|
| 稠密LLM（Dense LLMs） | 推理时所有参数均被激活 | 标准架构，兼容性强 |
| 专家混合LLM（MoE LLMs） | 由多个专用子网络构成，含路由机制，仅激活与输入相关的专家 | 参数效率更高，推理速度通常快于稠密模型 |

### 7. 并行化技术（多GPU推理支撑）
| 并行类型 | 拆分对象 | 核心逻辑 | 目的 |
|----------|----------|----------|------|
| 数据并行（Data） | 数据批次 | 将不同数据批分配到不同设备 | 为单设备无法容纳的大型数据集分配内存与计算资源 |
| 张量并行（Tensor） | 权重张量 | 按行或列将张量拆分到多设备 | 为单设备无法容纳的大型张量分配内存与计算资源 |
| 流水线并行（Pipeline） | 模型层（垂直拆分） | 并行拆分模型流水线的不同阶段 | 通过重叠不同模型阶段的计算提升吞吐量 |
| 上下文并行（Context） | 输入序列 | 将输入序列拆分为段分配到多设备 | 减少长序列输入的内存瓶颈 |
| 专家并行（Expert） | MoE模型的专家子网络 | 将各专家（小型子模型）分配到多设备 | 分散计算，支持更大模型并提升性能 |
| 全分片数据并行（Fully Sharded Data） | 数据、模型、优化器、梯度 | 多设备分片组件，并行处理数据，训练步后同步；按需从分片加载/重构参数，计算后立即丢弃 | 支持训练单设备内存无法容纳的超大型模型（同时分配模型参数与激活值） |

## 五、DigitalOcean相关工具支持
- **一键模型（1-click Models）**：与HuggingFace合作，将GPU Droplets与TGI（Text Generation Inference）优化的容器应用中的开源LLM集成，已内置张量并行、量化、FlashAttention、分页注意力等优化，用户可直接使用。
- **配套资源**：提供《大语言模型入门》教程，以及GPU性能优化、神经网络基础等前置知识参考链接，帮助用户快速上手。

## 六、总结与展望
- 推理优化是LLM领域的核心研究方向，需匹配快速发展的技术节奏，当前已出现“动态推理策略”（如OpenAI o1模型，为复杂数学、编程任务分配更多计算资源以提升性能）。
- 本文仅覆盖推理优化的核心内容，领域仍在快速演进，后续将有更多相关技术与实践分享。

## 七、扩展学习资源
- **博客**：NVIDIA技术博客《Mastering LLM Techniques: Inference Optimization》、HuggingFace《LLM Inference at scale with TGI》、Google Research《Looking back at speculative decoding》等。
- **论文**：《LLM-Inference-Bench》《Efficient Memory Management for LLM Serving with PagedAttention》《SARATHI》《Context Parallelism for Scalable Million-Token Inference》等。
- **演讲**：NVIDIA黄仁勋CES 2025主题演讲、Stanford CS 229S《Inference Math, Simulation, and AI Megaclusters》等。
- **GitHub资源**：Nvidia高性能LLM部署幻灯片、HuggingFace《search-and-learn》（开源模型推理扩展方案）。











