<img width="945" height="454" alt="image" src="https://github.com/user-attachments/assets/950c6fd0-d460-495e-88cd-2f18592bc1dc" />


<img width="633" height="477" alt="image" src="https://github.com/user-attachments/assets/4b548b08-7623-4c1f-aaa6-13cad54f7cb1" />


### 【从零到一】第24课 Decoding Strategies

#### 三个主要的推理逻辑
1. **Greedy Search**：选择最大可能性的下一个token
2. **Beam Search**：批量采样多个token推理结果，权重加合后返回最大值
3. **Top-K Sampling**：随机选择K个最高概率的token返回
4. **Temperature**：温度值取0.1~1来控制softmax函数的比例范围
5. **Top-P Sampling**

#### 概念流程
输入文本（如 *I have a dream*）→ **Tokenizer** 转换为输入token ids（如 40, 423, 257, 4320）→ **Large Language Model** 输出Logits（如 of、that、to、,、for 等候选token的概率分布）

#### Greedy Search
- 是很多语言模型推理的默认设置，也是推理速度最快的方式。
- 永远返回softmax之后概率最高的那个token。

#### Beam Search
（注：图中未完整展示，可结合逻辑理解：通过批量采样多个候选路径并加权，最终返回最优序列）


<img width="605" height="475" alt="image" src="https://github.com/user-attachments/assets/d0e05938-6316-4efc-a537-caaf0038678f" />

### 概念流程
输入文本（如 *I have a dream*）→ **Tokenizer** 转换为输入token ids（如 40, 423, 257, 4320）→ **Large Language Model** 输出Logits（如 of、that、to、,、for 等候选token的概率分布）

### Greedy Search
- 是很多语言模型推理的默认设置，也是**推理速度最快**的方式。
- 永远返回softmax之后概率最高的那个token。

### Beam Search
- 一次性推理N个可能性，这里的N称为**Beam数（光束数）**。
- 示例：若要求生成100个token，设置Beam数为3时，每次推理保存概率最高的3个token；待100轮推理完成后，计算3条序列的总概率和，返回总概率最大的100个token序列。
- 缺点：需等全部token推理完成后才能输出，无法实现“逐字输出”的效果。

### Top-K Sampling

top-k取样： 随机选取k个最高概率的预测的token

<img width="901" height="513" alt="image" src="https://github.com/user-attachments/assets/8bd5df03-38ae-41ba-bbfb-a8b3a98ee58a" />


### top-p sampling

top-p取样： 也叫做mucleus samplling

按照概率降序检查其中最可能的token，并且不断将他们添加到列表当中，直到总的概率超过阈值p，与top-k采样不同，核采样当中包含的token的数量可以在每一步当中有所变化，

这种变化通常会产生更具有多样性和创作型的输出

我们在《手写实战》系列的最后一课里实现过推理方法中的 temperature，原理非常简单，就是在 generate() 方法中算出 logits 之后再除以这个温度数值就可以了。

```python
def generate(self, idx, max_new_tokens=100, temperature=1.0, top_k=None):
    # idx is (B,T) array of indices in the current context | idx是当前上下文中的索引的(B,T)数组
    for _ in range(max_new_tokens):
        # Crop idx to the max size of our positional embeddings table | 将idx裁剪到我们位置嵌入表的最大大小
        idx_crop = idx[:, -self.context_length:]
        # Get predictions | 从模型中获取预测
        logits, loss = self.forward(idx_crop)
        # Get the last time step from logits where the dimensions of the logits are (B,T,C) | 从logits中获取最后一个时间步，其中logits的维度为(B,T,C)
        logits = logits[:, -1, :] / temperature  # Divide by temperature | 除以温度
        # optionally crop the logits to only the top k options | 可选地将logits裁剪为仅包含前k个选项
        if top_k is not None:
            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
            logits[logits < v[:, [-1]]] = -float('Inf')
        # Apply softmax to get probabilities | 应用softmax以获得概率
        probs = F.softmax(input=logits, dim=-1)
        # Sample from the probabilities' distribution. | 从概率分布中采样
        idx_next = torch.multinomial(input=probs, num_samples=1)
        # Append the sampled indexes idx_next to idx | 将采样的索引idx_next附加到idx
        idx = torch.cat((idx, idx_next), dim=1)
    return idx
```

<img width="543" height="343" alt="image" src="https://github.com/user-attachments/assets/07e784f0-7ea2-47fc-a54c-cbd8fd880d8d" />



<img width="525" height="471" alt="image" src="https://github.com/user-attachments/assets/bccb3f5e-4068-42c1-a521-f93351195e6d" />



<img width="520" height="433" alt="image" src="https://github.com/user-attachments/assets/471678c5-f60d-4002-9c94-a17c81e6c70e" />






