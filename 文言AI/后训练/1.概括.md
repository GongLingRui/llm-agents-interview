

| 模块名称     | 数据准备                               | 算法           | 模型     | 资源需求                     |
|--------------|----------------------------------------|----------------|----------|------------------------------|
| 预训练模块   | 预训练数据（千亿级token、多模态数据）| 语言模型训练   | 基础模型 | 1000+GPU、数百天训练周期     |
| 有益监督模块 | 标注用户指令（数百万人工标注的指令-回答数据） | 语言模型训练   | SFT模型  | 1-100GPU、数天训练周期       |
| 奖励建模模块 | 标注对比对（数万人工标注的对比数据）| 二分类模型     | 奖励模型 | 1-100GPU、数天训练周期       |
| 强化学习模块 | 用户指令（十万量级用户指令）| 强化学习方法   | RL模型   | 1-100GPU、数天训练周期       |


以下是整理后的文字内容：


### 垂类模型（如qwen2.5-7b）Post-training方案合理性探索
#### 一、背景分析
近年大语言模型（LLM）在通用自然语言理解与生成任务中表现突出，但通用LLM基于“大规模、多领域语料”训练，缺乏对特定垂直领域知识、任务特性的适配，导致行业应用中易出现**“知识失配”“表达偏差”“任务对齐不足”**等问题，制约实际业务落地效果。


#### 二、后训练（Post-training）的价值
为提升LLM在垂类场景下的性能与可控性，后训练已成为学术界、工业界的重要研究方向——通过针对性优化，让模型适配特定任务/价值观，实现性能提升与行为约束。


#### 三、后训练常用手段
后训练的核心优化方式包括：
1. 有监督微调（SFT）
2. 直接偏好优化（如DPO、KTO）
3. 强化学习式偏好优化（如GRPO）


#### 四、关联训练模块信息
垂类模型后训练涉及的核心模块及配置：
| 模块名称     | 数据准备                               | 算法           | 模型     | 资源需求                     |
|--------------|----------------------------------------|----------------|----------|------------------------------|
| 预训练模块   | 预训练数据（千亿级token、多模态数据）| 语言模型训练   | 基础模型 | 1000+GPU、数百天训练周期     |
| 有益监督微调模块 | 标注用户指令（数百万人工标注的指令-回答数据） | 语言模型训练   | SFT模型  | 1-100GPU、数天训练周期       |
| 奖励建模模块 | 标注对比对（数万人工标注的对比数据）| 二分类模型     | 奖励模型 | 1-100GPU、数天训练周期       |
| 强化学习模块 | 用户指令（十万量级用户指令）| 强化学习方法   | RL模型   | 1-100GPU、数天训练周期       |




# 0.1 开源项目与实践指南
## 1. 大模型Post-Training
- 评价：作为入门文章很合适，但嵌套较多
- 链接：https://qiankunli.github.io/2023/12/18/llm_post_training.html


## 2. 领域大模型-训练Trick&落地思考
- 来源：刘聪NLP
- 链接：https://zhuanlan.zhihu.com/p/648798461


## 3. Post-training 101
- 标题：A hitchhiker's guide into LLM post-training
- 链接：https://tokens-for-thoughts.notion.site/post-training-101


## 4. Hugging Face Alignment Handbook
- GitHub链接：https://github.com/huggingface/alignment-handbook
- 技术报告链接：https://arxiv.org/abs/2310.16944
- 核心价值：
  - 提供完整后训练流程：持续预训练→SFT→奖励建模→DPO/ORPO
  - 包含多个SOTA模型复现配方（Zephyr、SmollLM、StarChat等）
  - 支持分布式训练（DeepSpeed ZeRO-3）和参数高效微调（LoRA/QLoRA）


# 0.2 后训练快速入门框架

## 1. LLaMA-Factory
- 链接：https://github.com/hiyouga/LLaMA-Factory
- 支持能力（不同训练方式的覆盖情况）：

| Approach               | Full-tuning | Freeze-tuning | LoRA | QLoRA | OFT | QOFT |
|------------------------|-------------|---------------|------|-------|-----|------|
| Pre-Training           | ✅           | ✅             | ✅    | ✅     | ✅   | ✅    |
| Supervised Fine-Tuning | ✅           | ✅             | ✅    | ✅     | ✅   | ✅    |
| Reward Modeling        | ✅           | ✅             | ✅    | ✅     | ✅   | ✅    |
| PPO Training           | ✅           | ✅             | ✅    | ✅     | ✅   | ✅    |
| KTO Training           | ✅           | ✅             | ✅    | ✅     | ✅   | ✅    |
| ORPO Training          | ✅           | ✅             | ✅    | ✅     | ✅   | ✅    |
| SimPO Training         | ✅           | ✅             | ✅    | ✅     | ✅   | ✅    |


## 2. EasyR1
- 链接：https://github.com/hiyouga/EasyR1
- 支持模型：
  - Llama3/Qwen2/Qwen2.5/Qwen3 语言模型
  - Qwen2-VL/Qwen2.5-VL/Qwen3-VL 多模态语言模型
  - DeepSeek-R1 蒸馏模型
- 支持算法：
  - GRPO
  - DAPO（new）
  - Reinforce++
  - ReMax
  - RLOO
  - GSPO（new）
  - CISPO（new）

# 0.3 Paper

## 1. 《A Survey of Post-Training Scaling in Large Language Models》（ACL 2025）
- 定位：最全面的后训练综述
- 核心分类：将后训练方法系统性分为三大类
  1. **监督微调（SFT）**：涵盖指令生成、响应生成、数据合成等方向
  2. **强化学习（RL）**：包含合成奖励建模、环境反馈、自我反馈等方向
  3. **测试时计算（TTC）**：涉及采样、验证推理链、搜索、长上下文学习等方向
- 配套图示：分别提供了SFT、RL、TTC方法的分类架构图（Figure3-5）

<img width="519" height="125" alt="image" src="https://github.com/user-attachments/assets/2a028d30-7c5f-4c94-ba8f-edc8dc795772" />


## 2. 《A Comprehensive Survey of LLM Alignment Techniques》（2024）
- 核心价值：专注于对齐技术（RLHF、RLAIF、PPO、DPO等），详细分类各类对齐方法
- 适用场景：重点关注模型对齐和人类偏好学习

# 0.4 工业界技术报告
## 2024年大模型后训练总结
- 链接：https://zhuanlan.zhihu.com/p/987052830
- 核心价值：
  1. 系统总结9个主流开源模型的后训练方案：Llama3、Qwen2、Nemotron、AFM、Yi、GLM-4、Gemma2、DeepSeek-V2、Baichuan2
  2. 关键趋势识别：
     - 数据合成为主流方案
     - 拒绝采样（Rejection Sampling）+ LLM-as-Judge广泛应用
     - 重点能力需单独优化（代码、数学、推理、长上下文等）
     - 模型合并技术提升性能均衡性
  3. 强化学习技术对比：迭代式DPO、在线DPO、GRPO、PPO等


# 0.5 部分主流模型技术报告
## Llama 3
- 链接：https://arxiv.org/abs/2407.21783
- 核心方案：迭代式SFT+PPO（6轮）

<img width="496" height="247" alt="image" src="https://github.com/user-attachments/assets/0abc7dc6-343a-4a89-b71d-814ab512b96e" />

## DeepSeek-V3 :https://arxiv.org.pdf/2412.19437
- 链接：https://arxiv.org/abs/2407.21783
- 核心方案：迭代式SFT+PPO（6轮）


<img width="483" height="393" alt="image" src="https://github.com/user-attachments/assets/264e0350-05c3-4161-a3af-dfb1d1ea6dee" />

<img width="515" height="361" alt="image" src="https://github.com/user-attachments/assets/8f587980-15c9-4495-ac8f-74780c29ba68" />

## Qwen2.5
- 链接：https://arxiv.org/pdf/2412.15115
- SFT -> DPO ->GRPO






