# 1. LLM post-training的分类
在《A Survey of Post-Training Scaling in Large Language Models》中，“后训练”被系统划分为三大类：
1. **监督微调（SFT）**
2. **强化学习/对齐（RL / alignment）**
3. **测试时计算/推理增强（TTC, test-time compute）**

该分类的覆盖范围：
- 包含传统的指令微调、监督学习；
- 涵盖近年兴起的基于偏好/奖励的对齐训练；
- 也包含推理/部署阶段通过计算策略提升能力的做法（如验证性推理链、检索增强、长上下文学习等）。


# 1.1 各方法的技术特点对比



### 一、LLM 后训练（LLM post-training）
#### 1. 有监督微调（SFT）
按**参数更新范围**分为两类：
- **全参数微调（Full FT）**
  - 所有模型参数都更新；
  - 计算、显存消耗大；
  - 适合训练 base→instruct，或高质量闭源模型的内部训练。

- **参数高效微调（PEFT）**
  - 只训练很少一部分参数，严格保持 pretrain 权重大部分冻结；
  - 包含以下子类型：
    1. **Adapter类**：Adapter Tuning
    2. **Prefix Tuning类**：Prefix Tuning、Prompt Tuning、P-tuning、P-Tuning v2
    3. **LoRA类**：LoRA、AdaLoRA、QLoRA


#### 2. 偏好优化 / RLxF
分为两种优化模式：
- **直接偏好优化**
  - 不需要价值函数（value model）、rollout / environment；
  - 通常不依赖参考模型（或弱依赖）；
  - 训练稳定、成本低、易扩展；
  - 优化目标直接从“偏好数据”推导（pairwise 或 pointwise）；
  - 子类型：
    - DPO：通过对比 preferred vs. rejected 的 log-ratio 直接拟合偏好；
    - KTO：在 DPO 框架加入“损失加权”，让模型更关注严重错误（损失不对称）；
    - SimPO：取消 reference model 和 log-ratio，训练更稳健，适合大规模自对齐。

- **强化学习式偏好优化**
  - 需要 reward / advantage；
  - 有 value baseline（但通常用 group-based 代替）；
  - 训练更强，但更昂贵；
  - 可进行 sequence-level 优化，更适合复杂任务（推理、工具调用）；
  - 子类型：
    - GRPO：用 group-based baseline 代替 value model（Qwen、DeepSeek 使用），训练更稳定，适合长序列、多步骤推理；
    - DAPO：包含 Decoupled Clip（解耦的裁剪策略）、Dynamic Sampling（动态采样）；
    - GSPO：引入 sequence-level importance sampling，更适合长序列任务与 reasoning。


### 二、推理时计算增强（TTC）
训练不变，仅通过推理时花更多算力让模型“更聪明”，分为5类：

#### 1. Sampling-based TTC（采样类）
- 核心：通过“增加生成样本数量”提升最终输出质量；
- 子类型：多样采样（Multi-sample Sampling）
  - 例如 top-k、top-p、temperature、beam search；
  - 核心思想：多生成几个答案，再选最好的；
  - 常用于：创意生成、开放式问答。


#### 2. Search-based TTC（搜索类）
- 核心：推理过程中使用搜索算法寻找更优解，让模型生成“多个步骤路径”，选择更优路径的解；
- 子类型：
  - Simple Search（简单树搜索）：BFS / DFS 思想，小规模搜索路径；
  - Monte Carlo Tree Search（MCTS）：用于数学推理、代码推理等任务（DeepMind AlphaCode 也使用类似技术）；
  - Beam Search（束搜索）：同时保留多个候选序列进行扩展。


#### 3. Reasoning-based TTC（推理链类）
- 核心：利用“思维链”提升模型的可解释推理能力，让模型“多想一步、多写几条链条”，再选更靠谱的；
- 子类型：
  - CoT（Chain-of-Thought）：显式生成推理步骤；
  - Self-Consistency（自一致性）：生成多条 CoT，投票选最佳答案；
  - Tree-of-Thought（ToT）：思维链升级为“思维树”；
  - Graph-of-Thought（GoT）：思维链结构，用于复杂规划。


#### 4. Self-Verification（自校验类）
- 核心：推理完后再让模型进行一次检查，模型自己当“老师”，检查生成内容是否正确；
- 子类型：
  - Prof Verification（推理验证）：由模型检查自己的答案逻辑；
  - Self-Critique（自我反思）：让模型先“反思错误”再修改（如 DeepSeek 的 self-evolved）；
  - Answer Reranking（重排序）：生成多个答案→自评分→选择最优。


#### 5. Context-based TTC（上下文增强类）
- 核心：通过扩展输入上下文，提高推理质量；输入更丰富→推理更准确；
- 子类型：
  - Long-context Reasoning（长上下文推理）：使用长 context window（100K-1M tokens）；
  - Retrieval-Augmented Inference（检索增强推理）：RAG 但不改变模型参数，检索更多相关信息作为 context；
  - In-context Learning（ICL）：Zero-shot / Few-shot prompts。

# LLM 后训练与推理增强全分类框架（结构化可复制版）
## 一、LLM 后训练（LLM post-training）
### 1. 有监督微调（SFT）
#### （1）全参数微调（Full FT）
- 所有模型参数均更新
- 计算、显存消耗大
- 适用场景：base→instruct 训练、高质量闭源模型内部训练

#### （2）参数高效微调（PEFT）
- 仅训练少量参数，大部分 pretrain 权重冻结
- 子类型：
  - Adapter类：Adapter Tuning
  - Prefix Tuning类：Prefix Tuning、Prompt Tuning、P-tuning、P-Tuning v2
  - LoRA类：LoRA、AdaLoRA、QLoRA

### 2. 偏好优化 / RLxF
#### （1）直接偏好优化
- 无需价值函数（value model）、rollout / environment
- 弱依赖或不依赖参考模型
- 训练稳定、成本低、易扩展
- 优化目标从“偏好数据”（pairwise / pointwise）推导
- 子类型：DPO、KTO、SimPO

#### （2）强化学习式偏好优化
- 需要 reward / advantage，含 value baseline（常用 group-based 替代）
- 训练效果更强，成本更高
- 支持 sequence-level 优化，适配复杂任务（推理、工具调用）
- 子类型：GRPO、DAPO、GSPO

## 二、推理时计算增强（TTC）
### 1. Sampling-based TTC（采样类）
- 核心：增加生成样本数量提升输出质量
- 子类型：多样采样（top-k、top-p、temperature、beam search）
- 适用场景：创意生成、开放式问答

### 2. Search-based TTC（搜索类）
- 核心：通过搜索算法寻找更优解，生成多步骤路径并选最优
- 子类型：Simple Search（BFS/DFS）、Monte Carlo Tree Search（MCTS）、Beam Search
- 适用场景：数学推理、代码推理

### 3. Reasoning-based TTC（推理链类）
- 核心：借助思维链提升可解释推理能力，生成多链条后选最优
- 子类型：CoT（Chain-of-Thought）、Self-Consistency、Tree-of-Thought（ToT）、Graph-of-Thought（GoT）

### 4. Self-Verification（自校验类）
- 核心：推理后由模型自身检查内容正确性
- 子类型：Prof Verification、Self-Critique、Answer Reranking

### 5. Context-based TTC（上下文增强类）
- 核心：扩展输入上下文提升推理质量
- 子类型：Long-context Reasoning、Retrieval-Augmented Inference、In-context Learning（ICL）

```
LLM 后训练与推理增强
├─ 一、LLM 后训练（LLM post-training）
│  ├─ 1. 有监督微调（SFT）
│  │  ├─ （1）全参数微调（Full FT）
│  │  │  ├─ 所有模型参数均更新
│  │  │  ├─ 计算、显存消耗大
│  │  │  └─ 适用：base→instruct 训练、闭源模型内部训练
│  │  └─ （2）参数高效微调（PEFT）
│  │     ├─ 仅训练少量参数，冻结大部分 pretrain 权重
│  │     ├─ Adapter类：Adapter Tuning
│  │     ├─ Prefix Tuning类：Prefix Tuning、Prompt Tuning、P-tuning、P-Tuning v2
│  │     └─ LoRA类：LoRA、AdaLoRA、QLoRA
│  └─ 2. 偏好优化 / RLxF
│     ├─ （1）直接偏好优化
│     │  ├─ 无需价值函数/rollout；弱依赖参考模型
│     │  ├─ 训练稳定、成本低、易扩展
│     │  ├─ 从“偏好数据”推导优化目标
│     │  ├─ DPO：对比 preferred/rejected 的 log-ratio 拟合偏好
│     │  ├─ KTO：DPO+损失加权，关注严重错误
│     │  └─ SimPO：取消参考模型，训练更稳健
│     └─ （2）强化学习式偏好优化
│        ├─ 需要 reward；用 group-based 替代 value baseline
│        ├─ 训练强但昂贵；支持 sequence-level 优化
│        ├─ GRPO：group-based 替代 value model（Qwen/DeepSeek用）
│        ├─ DAPO：Decoupled Clip+Dynamic Sampling
│        └─ GSPO：sequence-level 重要性采样
└─ 二、推理时计算增强（TTC）
   ├─ 1. Sampling-based TTC（采样类）
   │  ├─ 核心：增加样本数量提升质量
   │  └─ 子类型：多样采样（top-k/top-p等）
   ├─ 2. Search-based TTC（搜索类）
   │  ├─ 核心：搜索算法找多路径最优解
   │  ├─ Simple Search（BFS/DFS）
   │  ├─ MCTS（数学/代码推理）
   │  └─ Beam Search
   ├─ 3. Reasoning-based TTC（推理链类）
   │  ├─ 核心：思维链提升推理能力
   │  ├─ CoT（显式推理步骤）
   │  ├─ Self-Consistency（多CoT投票）
   │  ├─ ToT（思维树）
   │  └─ GoT（思维图）
   ├─ 4. Self-Verification（自校验类）
   │  ├─ 核心：模型自检生成内容
   │  ├─ Prof Verification（逻辑检查）
   │  ├─ Self-Critique（反思修改）
   │  └─ Answer Reranking（多答案重排序）
   └─ 5. Context-based TTC（上下文增强类）
      ├─ 核心：扩展上下文提升推理
      ├─ Long-context Reasoning（超长上下文）
      ├─ 检索增强推理（RAG）
      └─ ICL（Zero/Few-shot）
```



### 一、后训练方法总结表格
| 类别          | 优势               | 局限性                     | 可扩展性               |
|---------------|--------------------|----------------------------|------------------------|
| SFT（监督微调） | 实现简单，数据可控 | 依赖数据质量，泛化能力有限 | 中等（受数据获取限制） |
| RLxF（偏好优化）| 能对齐复杂偏好，自适应优化 | 训练不稳定，可能奖励欺骗 | 高（尤其是自我反馈）|
| TTC（推理时增强）| 无需重训练，灵活高效 | 推理成本高，受模型能力上限约束 | 高（可动态调节计算） |


### 二、后训练方法核心说明
大模型预训练后的后训练方法分为三大类：**Supervised Fine-Tuning (SFT)、Reinforcement Learning from Feedback (RLxF)、Test-time Compute (TTC)**。
- **SFT**：用（或生成）高质量 instruction-response 数据，让模型按指令输出——对特定任务/domain 适应性强；
- **RLxF**：通过奖励/反馈（human、synthetic、environment-based 等）优化模型行为，更适合复杂/open-ended/interactive/多步任务；
- **TTC**：利用更强/更智能的推理/采样/搜索/验证机制，在推理阶段提升模型输出质量，无需再训练或仅少量调整。


### 三、三类方法的优劣势
- **SFT**：易于理解/实现/定制，但数据依赖高；
- **RLxF**：弹性与潜力大，但 reward/feedback 设计复杂、挑战多；
- **TTC**：灵活低成本，但对模型本身知识/表示（representation）改变有限。


### 四、实际应用趋势
很多系统会将这几种方法组合起来（例如 SFT + RLxF + 推理时的采样/verification）——这是目前 post-training scaling 的主流趋势。


















