
<img width="316" height="130" alt="image" src="https://github.com/user-attachments/assets/c767ad37-3d91-4224-9895-7a7b9653068b" />


### 一、传统强化学习对齐方法（如RLHF）的局限
在RLHF中，需要**显式的奖励模型（Reward Model, RM）**：
- RM是独立训练的神经网络，用于给每个回答打分（如0-1分）；
- 以该分数作为强化学习的奖励信号。


### 二、直接偏好优化的核心思想
DPO/KTO/SimPO等方法的核心：**不训练奖励模型，直接利用语言模型自身的输出概率，通过数学变换“隐式”构造奖励函数**。


### 三、主流直接偏好优化方法
1. **DPO（Direct Preference Optimization）**
   - 核心：“用偏好对直接训练模型、不需要奖励模型、不需要RL”。

2. **KTO（Kahneman-Tversky Optimization）**
   - 核心：在DPO框架上加入**行为经济学的损失加权**，让模型更关注错误、实现线性化损失。

3. **SimPO（Simple Preference Optimization）**
   - 核心：简化DPO，不再需要reference model、不需要log-ratio，让优化更稳健。


### 四、小结
直接偏好优化的核心：**不训练奖励模型，直接利用语言模型自身输出概率，通过数学变换“隐式”构造奖励函数**。


