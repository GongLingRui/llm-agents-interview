


### 一、SFT 核心概述
- **定位**：指令与数据合成为核心的路线
- **工作机制**：
  1. 以有监督学习将预训练模型调整为可遵从指令/完成特定任务的模型，常用人工标注指令-响应对或合成数据集微调；
  2. 参数高效方案（PEFT：LoRA/QLoRA等）常与全参微调结合，用于资源受限下快速部署指令化模型（LoRA使用较多）；
- **优缺点**：
  - 优点：工程成熟、数据驱动、易并行化、能针对单一垂类任务快速提升效果；
  - 缺点：对高质量标注/合成数据依赖强；纯SFT在复杂偏好/价值约束场景下可能不够稳健。


### 二、SFT 数据讨论（质量、数量、多样性）
1. **质量>数量**：精选小规模高质量数据（1K-10K）可超越大规模低质量数据（50K-300K）；
2. **边际收益递减**：数据量超过临界点后，性能提升停滞甚至下降；
3. **多样性权衡**：需在质量和多样性间找平衡，纯质量或纯多样性都不是最优；
- **相关研究**：
  - **DEITA（ICLR 2024）**：仅用6K高质量自动筛选样本即可达到/超越30万样本的效果，数据减少100倍、性能不降反升；
  - **LIMA（Meta AI）**：对齐的缩放律不仅取决于数量，更取决于“保持高质量回答前提下的提示多样性”；高多样性（如Filtered Stack Exchange）>低多样性（如wikiHow），即使两者都是高质量数据。

<img width="686" height="369" alt="image" src="https://github.com/user-attachments/assets/a7efa27e-66f7-4053-94ee-9ded88b89727" />

<img width="565" height="392" alt="image" src="https://github.com/user-attachments/assets/6a23e2bd-fdf2-404f-ae49-2234d8631b48" />


### 三、模型选择（Base/Instruct/Chat）
#### 1. 三者关系
`Base模型（文本续写）` →（指令微调）→ `Instruct模型（遵循指令）` →（对话优化）→ `Chat模型（多轮对话）`

#### 2. 实例对比（输入：“写一首关于春天的诗”）
- Base模型：可能续写“写一首关于春天的诗的方法有很多种...”（作文本续写）；
- Instruct模型：直接输出一首关于春天的诗；
- Chat模型：输出诗，并可能问“您喜欢这首诗吗？需要我调整风格吗？”。

#### 3. 模型对比表
| 模型类型 | 训练方式               | 特点       | 优点               | 缺点               | 使用场景               |
|----------|------------------------|------------|--------------------|--------------------|------------------------|
| Base     | 仅预训练               | 原始能力强 | 泛化性好、可控     | 不懂指令、输出混乱 | 研发、微调、专业模型   |
| Instruct | Base + SFT             | 任务型     | 执行力强、精确     | 不擅长多轮对话     | 翻译、抽取、代码生成   |
| Chat     | SFT + 偏好对齐（DPO/GRPO） | 对话型     | 交互自然、安全性高 | 精确执行不如Instruct | 助手、客服、聊天       |

#### 4. 选择建议
- 研究和定制化微调→Base模型；
- 明确的任务完成→Instruct模型；
- 交互式应用和助手→Chat模型。


### 四、SFT 总结
LLM的有监督微调是技术选择与资源约束的权衡过程：
1. 全参微调适合资源充足、追求极致效果的场景；
2. PEFT方法是当前主流，其中LoRA/QLoRA性价比最高；
3. 数据选择关键点：数据质量>数据数量；
4. 模型选择关键点：要研发选Base；要做任务选Instruct；要做产品对话选Chat。


### 五、附：快问快答
1. **Q1：LoRA为什么推理时没有额外延迟？**

   A：LoRA训练完成后，可将低秩矩阵ΔW直接加到原权重W₀上，形成新权重W'=W₀+ΔW；推理时只需使用W'，无需额外矩阵运算，因此无延迟。

3. **Q2：为什么PEFT不容易遗忘？**

   A：PEFT冻结了原模型大部分参数（保留预训练通用知识），仅小部分参数学习任务特定知识；即使这些参数过拟合，原模型通用能力不受影响。

4. **Q3：什么情况下PEFT效果不如全参微调？**
   A：

   ①任务与预训练差异巨大（基座模型无该领域知识，PEFT有限参数学不足）；

   ②数据量极充足（≥50K，全参微调能充分利用数据，PEFT参数成瓶颈）；

   但实践中LoRA/QLoRA在多数场景能达到全参95%以上效果，优先推荐PEFT。

5. **Q4：如何评估微调效果？**
   A：三个维度：

   ①任务指标（准确率、F1、BLEU等）；

   ②通用能力保留（在MMLU等benchmark测试）；

   ③实际业务效果（A/B测试、用户反馈、badcase分析）；

   需建立完整评估体系，不能只看单一指标。


