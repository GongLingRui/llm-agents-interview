# Agent RL é›¶åŸºç¡€å…¥é—¨æ•™ç¨‹

> é¢å‘å®Œå…¨é›¶åŸºç¡€çš„åŒå­¦ï¼Œä»ç”Ÿæ´»ä¾‹å­åˆ°ä»£ç å®ç°

---

## ç›®å½•

1. [ä»€ä¹ˆæ˜¯æ™ºèƒ½ä½“ï¼ˆAgentï¼‰ï¼Ÿ](#1-ä»€ä¹ˆæ˜¯æ™ºèƒ½ä½“agent)
2. [ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Ÿ](#2-ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ rl)
3. [æ ¸å¿ƒæ¦‚å¿µè¯¦è§£](#3-æ ¸å¿ƒæ¦‚å¿µè¯¦è§£)
4. [æ‰‹æŠŠæ‰‹çœ‹ä»£ç ](#4-æ‰‹æŠŠæ‰‹çœ‹ä»£ç )
5. [å®æˆ˜æ¡ˆä¾‹](#5-å®æˆ˜æ¡ˆä¾‹)
6. [å­¦ä¹ è·¯çº¿å›¾](#6-å­¦ä¹ è·¯çº¿å›¾)

---

# 1. ä»€ä¹ˆæ˜¯æ™ºèƒ½ä½“ï¼ˆAgentï¼‰ï¼Ÿ

## 1.1 ä»ç”Ÿæ´»ä¾‹å­ç†è§£

### ä¾‹å­1ï¼šç©æ¸¸æˆçš„å°ç‹—

```
æƒ³è±¡ä½ æ­£åœ¨è®­ç»ƒä¸€åªå°ç‹—ç©"æ‰¾çƒ"æ¸¸æˆï¼š

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                        â”‚
â”‚   ğŸ¶ å°ç‹—                              â”‚
â”‚                                        â”‚
â”‚   ä½ è¯´ï¼š"å»æ‰¾çƒï¼"                     â”‚
â”‚      â†“                                â”‚
â”‚   å°ç‹—è·‘å‘è‰ä¸› â†’ æ²¡æ‰¾åˆ°ï¼ˆä½ ä¸è¯´å•¥ï¼‰     â”‚
â”‚   å°ç‹—è·‘å‘æ ‘ä¸‹ â†’ æ‰¾åˆ°äº†ï¼ˆä½ ç»™å®ƒé›¶é£ŸğŸ¦´ï¼‰ â”‚
â”‚      â†“                                â”‚
â”‚   å°ç‹—å­¦ä¼šäº†ï¼šå»æ ‘ä¸‹èƒ½æ‰¾åˆ°çƒï¼         â”‚
â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼š
- å°ç‹— = æ™ºèƒ½ä½“ï¼ˆAgentï¼‰
- "æ‰¾çƒ" = ä»»åŠ¡
- è‰ä¸›ã€æ ‘ä¸‹ = ç¯å¢ƒï¼ˆEnvironmentï¼‰
- é›¶é£Ÿ = å¥–åŠ±ï¼ˆRewardï¼‰
- å°ç‹—å­¦ä¼šäº†ç­–ç•¥ = è®­ç»ƒ
```

### ä¾‹å­2ï¼šå¤–å–éª‘æ‰‹

```
ğŸƒ å¤–å–éª‘æ‰‹ä¹Ÿæ˜¯ä¸ªæ™ºèƒ½ä½“ï¼š

çŠ¶æ€ï¼šå½“å‰ä½ç½®ã€ç›®çš„åœ°ã€å¤©æ°”ã€äº¤é€šçŠ¶å†µ
      â†“
åŠ¨ä½œï¼šé€‰æ‹©å“ªæ¡è·¯çº¿
      â†“
å¥–åŠ±ï¼šå‡†æ—¶é€è¾¾=å¥½è¯„+æ”¶å…¥ï¼Œè¿Ÿåˆ°=å·®è¯„-æ”¶å…¥
      â†“
ç›®æ ‡ï¼šå­¦ä¼šé€‰æ‹©æœ€ä¼˜è·¯çº¿ï¼Œèµšæ›´å¤šé’±

éª‘æ‰‹é€šè¿‡ä¸æ–­é€é¤ï¼ˆè¯•é”™ï¼‰ï¼Œå­¦ä¼šäº†ï¼š
- å“ªæ€•è¿‘è·¯å µè½¦ï¼Œè¿œä¸€ç‚¹çš„è·¯å¯èƒ½æ›´å¿«
- ä¸‹é›¨å¤©è¦éª‘æ…¢ä¸€ç‚¹
- æŸäº›å°åŒºçš„é—¨ä¸å¥½æ‰¾
```

### æ€»ç»“ï¼šæ™ºèƒ½ä½“çš„ç‰¹å¾

```
æ™ºèƒ½ä½“ = èƒ½æ„ŸçŸ¥ç¯å¢ƒ + åšå†³ç­– + ä»ç»éªŒä¸­å­¦ä¹ 

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                      â”‚
â”‚   ğŸ‘€ æ„ŸçŸ¥  â†’  ğŸ§  åšå†³ç­–  â†’  âœ‹ è¡ŒåŠ¨   â”‚
â”‚      â†“            â†“           â†“      â”‚
â”‚   çœ‹åˆ°çŠ¶æ€      æ€è€ƒæ€ä¹ˆåŠ     æ‰§è¡ŒåŠ¨ä½œ  â”‚
â”‚      â†“            â†“           â†“      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   â”‚  ç¯å¢ƒï¼ˆä¸–ç•Œï¼‰          â”‚â”€â”€â–º å¥–åŠ±/æƒ©ç½šâ”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚            â†‘                           â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚              ä»ç»éªŒä¸­å­¦ä¹               â”‚
â”‚                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# 2. ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Ÿ

## 2.1 ä¸ºä»€ä¹ˆè¦ç”¨å¼ºåŒ–å­¦ä¹ ï¼Ÿ

### ä¼ ç»Ÿè®­ç»ƒæ–¹æ³•çš„é—®é¢˜

```
é—®é¢˜ï¼š "1+1ç­‰äºå¤šå°‘ï¼Ÿ"

âŒ æ­»è®°ç¡¬èƒŒï¼ˆç›‘ç£å­¦ä¹ ï¼‰ï¼š
æ¨¡å‹ï¼š"2" â†’ âœ… å¯¹äº†
æ¨¡å‹ï¼š"3" â†’ âŒ é”™äº†
é—®é¢˜ï¼šåªèƒ½èƒŒè¯µå·²çŸ¥ç­”æ¡ˆï¼Œé‡åˆ°æ–°é¢˜ä¸ä¼š

âœ… å¼ºåŒ–å­¦ä¹ ï¼š
æ¨¡å‹å°è¯•ï¼š"1+1=1.5" â†’ âŒ é”™äº†ï¼Œç»™-1åˆ†
æ¨¡å‹å°è¯•ï¼š"1+1=2" â†’ âœ… å¯¹äº†ï¼Œç»™+1åˆ†
æ¨¡å‹å°è¯•ï¼š"1+1=1.8" â†’ æ¥è¿‘äº†ï¼Œç»™+0.5åˆ†
...
æ¨¡å‹å­¦ä¼šäº†ï¼šè¦å¾€"2"è¿™ä¸ªæ–¹å‘è°ƒæ•´
```

### å¼ºåŒ–å­¦ä¹ çš„æœ¬è´¨

```
å¼ºåŒ–å­¦ä¹  = é€šè¿‡è¯•é”™ + å¥–åŠ±åé¦ˆ æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                        â”‚
â”‚   å°è¯•è¡ŒåŠ¨ â†’ å¾—åˆ°åé¦ˆ â†’ è°ƒæ•´ç­–ç•¥        â”‚
â”‚      â†“           â†“           â†“        â”‚
â”‚   (åšé€‰æ‹©)   (å¥½/å?)   (æ”¹è¿›)         â”‚
â”‚                                        â”‚
â”‚   å°±åƒå­¦éª‘è‡ªè¡Œè½¦ï¼š                     â”‚
â”‚   1. éª‘ä¸Šå» â†’ æ‘”å€’ â†’ ç–¼ç—›ï¼ˆæƒ©ç½šï¼‰       â”‚
â”‚   2. å†è¯• â†’ å¹³è¡¡äº†ä¸€ç‚¹ â†’ æ²¡æ‘”ï¼ˆå¥–åŠ±ï¼‰   â”‚
â”‚   3. å†è¯• â†’ éª‘å¾—æ›´ç¨³ â†’ æˆåŠŸï¼ˆå¤§å¥–åŠ±ï¼‰   â”‚
â”‚   4. å­¦ä¼šäº†éª‘è‡ªè¡Œè½¦ï¼                  â”‚
â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# 3. æ ¸å¿ƒæ¦‚å¿µè¯¦è§£

## 3.1 MDPï¼ˆé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼‰

è¿™æ˜¯å¼ºåŒ–å­¦ä¹ çš„**æ•°å­¦æ¡†æ¶**ï¼Œä½†å…¶å®å°±æ˜¯æŠŠ"åšå†³ç­–"è¿™ä»¶äº‹å½¢å¼åŒ–åœ°æè¿°å‡ºæ¥ã€‚

### ç”¨é€šä¿—çš„è¯ç†è§£MDP

```
æƒ³è±¡ä½ åœ¨ç©ä¸€ä¸ªé—¯å…³æ¸¸æˆï¼š

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                        â”‚
â”‚   ã€çŠ¶æ€ Stateã€‘                       â”‚
â”‚   ä½ åœ¨ç¬¬1å…³ï¼Œæœ‰3æ¡å‘½ï¼Œ100åˆ†           â”‚
â”‚                                        â”‚
â”‚   ã€åŠ¨ä½œ Actionã€‘                      â”‚
â”‚   ä½ å¯ä»¥ï¼šè·³è·ƒã€å°„å‡»ã€è¹²ä¸‹             â”‚
â”‚                                        â”‚
â”‚   ã€è½¬ç§» Transitionã€‘                  â”‚
â”‚   ä½ é€‰æ‹©"è·³è·ƒ" â†’ åˆ°è¾¾å¹³å°              â”‚
â”‚   ä½ é€‰æ‹©"å°„å‡»" â†’ æ‰“ä¸­æ•Œäºº+100åˆ†        â”‚
â”‚                                        â”‚
â”‚   ã€å¥–åŠ± Rewardã€‘                      â”‚
â”‚   è·³åˆ°å¹³å° = +10åˆ†                     â”‚
â”‚   è¢«å­å¼¹æ‰“ä¸­ = -20åˆ†                   â”‚
â”‚   åˆ°è¾¾ç»ˆç‚¹ = +100åˆ†                    â”‚
â”‚                                        â”‚
â”‚   ã€ç­–ç•¥ Policyã€‘                      â”‚
â”‚   é‡åˆ°æ•Œäººæ—¶å°„å‡»ï¼Œé‡åˆ°æ‚¬å´–æ—¶è·³è·ƒ       â”‚
â”‚   è¿™æ˜¯ä½ å­¦ä¼šçš„"ç©æ³•"                  â”‚
â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### MDPçš„æ•°å­¦è¡¨ç¤ºï¼ˆç®€å•ç†è§£ï¼‰

```python
# ä¼ªä»£ç ï¼šMDPçš„å¾ªç¯

çŠ¶æ€ = å½“å‰æƒ…å†µ
while æ²¡æœ‰ç»“æŸ:
    # æ ¹æ®å½“å‰çŠ¶æ€ï¼Œé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ
    åŠ¨ä½œ = ç­–ç•¥(çŠ¶æ€)

    # æ‰§è¡ŒåŠ¨ä½œï¼Œç¯å¢ƒç»™å‡ºåé¦ˆ
    æ–°çŠ¶æ€, å¥–åŠ± = ç¯å¢ƒ.æ‰§è¡Œ(åŠ¨ä½œ)

    # ä»ç»éªŒä¸­å­¦ä¹ 
    ç­–ç•¥.æ›´æ–°(çŠ¶æ€, åŠ¨ä½œ, å¥–åŠ±, æ–°çŠ¶æ€)

    # è¿›å…¥ä¸‹ä¸€è½®
    çŠ¶æ€ = æ–°çŠ¶æ€
```

## 3.2 ç­–ç•¥ï¼ˆPolicyï¼‰

**ç­–ç•¥å°±æ˜¯"æ€ä¹ˆåšå†³å®šçš„è§„åˆ™"**

```
ç”Ÿæ´»ä¸­çš„ç­–ç•¥ï¼š

ğŸš— å¼€è½¦ç­–ç•¥ï¼š
   - çº¢ç¯ â†’ åœ
   - ç»¿ç¯ â†’ è¡Œ
   - å‰è½¦å‡é€Ÿ â†’ å‡é€Ÿ
   - è¦è½¬å¼¯ â†’ æ‰“è½¬å‘ç¯

ğŸ¤– AIçš„ç­–ç•¥ï¼š
   åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œç­–ç•¥å°±æ˜¯ä¸€ä¸ªå‡½æ•°ï¼š
   è¾“å…¥ï¼šå½“å‰çŠ¶æ€
   è¾“å‡ºï¼šè¦åšä»€ä¹ˆåŠ¨ä½œ

   ç­–ç•¥(çŠ¶æ€) = åŠ¨ä½œ

   ä¾‹å¦‚ï¼š
   ç­–ç•¥("å‰æ–¹æœ‰éšœç¢ç‰©") = "ç»•è¡Œ"
   ç­–ç•¥("é—®é¢˜éœ€è¦è®¡ç®—") = "ä½¿ç”¨ä»£ç å·¥å…·"
```

### ä»£ç ä¸­çš„ç­–ç•¥

```python
# åœ¨é¡¹ç›®ä¸­ï¼Œç­–ç•¥å°±æ˜¯LLMæ¨¡å‹æœ¬èº«

class Policy:
    def __init__(self, model):
        self.model = model

    def decide_action(self, state):
        """
        æ ¹æ®çŠ¶æ€å†³å®šåŠ¨ä½œ
        """
        # æŠŠçŠ¶æ€å˜æˆæç¤ºè¯
        prompt = f"å½“å‰çŠ¶æ€ï¼š{state}\nè¯·é€‰æ‹©ä¸‹ä¸€æ­¥åŠ¨ä½œï¼š"

        # æ¨¡å‹ç”Ÿæˆ
        action = self.model.generate(prompt)

        return action

# ä½¿ç”¨
policy = Policy(llm_model)
state = "è¿™æ˜¯ä¸€ä¸ªæ•°å­¦é¢˜ï¼š123 Ã— 456 = ?"
action = policy.decide_action(state)
# å¯èƒ½è¾“å‡ºï¼š"ä½¿ç”¨Pythonä»£ç è®¡ç®—"
```

## 3.3 å¥–åŠ±ï¼ˆRewardï¼‰

**å¥–åŠ±å°±æ˜¯å‘Šè¯‰æ™ºèƒ½ä½“"ä½ åšå¾—æ€ä¹ˆæ ·"**

```
å¥–åŠ±è®¾è®¡çš„é‡è¦æ€§ï¼š

âŒ ä¸å¥½å¥–åŠ±è®¾è®¡ï¼š
   åªå¥–åŠ±"ç­”æ¡ˆå¯¹äº†"
   é—®é¢˜ï¼šæ™ºèƒ½ä½“å¯èƒ½å­¦ä¼š"çŒœç­”æ¡ˆ"

âœ… å¥½å¥–åŠ±è®¾è®¡ï¼š
   - ç­”æ¡ˆå¯¹äº† = +1åˆ†
   - ä½¿ç”¨äº†å·¥å…· = +0.3åˆ†ï¼ˆé¼“åŠ±å·¥å…·ä½¿ç”¨ï¼‰
   - æ¨ç†æ­¥éª¤æ¸…æ™° = +0.2åˆ†
   - æœç´¢ç›¸å…³ = +0.2åˆ†
   - æœç´¢ä¸ç›¸å…³ = -0.1åˆ†
```

### ä»£ç ä¸­çš„å¥–åŠ±å‡½æ•°

```python
# æ¥è‡ªR1-Searcheré¡¹ç›®

class RewardCalculator:
    def calculate_reward(self, question, answer, ground_truth):
        """
        è®¡ç®—å¥–åŠ±åˆ†æ•°
        """
        reward = 0.0

        # 1. ç­”æ¡ˆæ­£ç¡®æ€§æ£€æŸ¥ï¼ˆæœ€é‡è¦ï¼Œ70%ï¼‰
        if self.answer_correct(answer, ground_truth):
            reward += 1.0
        else:
            reward += 0.0

        # 2. æ ¼å¼æ£€æŸ¥ï¼ˆæ ¼å¼é”™ä¼šæ‰£å¾ˆå¤šåˆ†ï¼‰
        if not self.valid_format(answer):
            reward -= 2.0  # ä¸¥é‡æƒ©ç½šï¼
            return reward  # æ ¼å¼é”™äº†å°±ä¸ç”¨çœ‹äº†

        # 3. æ˜¯å¦ä½¿ç”¨äº†æœç´¢ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰
        if self.has_search(answer):
            reward += 0.3

        return reward

    def answer_correct(self, pred, truth):
        """æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦æ­£ç¡®"""
        # ä½¿ç”¨F1åˆ†æ•°ï¼Œä¸æ˜¯ä¸¥æ ¼åŒ¹é…
        return f1_score(pred, truth) > 0.9

    def valid_format(self, answer):
        """æ£€æŸ¥æ ¼å¼"""
        has_thinking = "<thinking>" in answer
        has_search = "<search>" in answer
        has_answer = "<answer>" in answer
        return all([has_thinking, has_search, has_answer])
```

## 3.4 ä»·å€¼å‡½æ•°ï¼ˆValue Functionï¼‰

**ä»·å€¼å‡½æ•°è¯„ä¼°"è¿™ä¸ªçŠ¶æ€æœ‰å¤šå¥½"**

```
ç”Ÿæ´»ä¸­çš„ä»·å€¼åˆ¤æ–­ï¼š

ğŸ® æ¸¸æˆä¸­çš„ä»·å€¼åˆ¤æ–­ï¼š
   "æˆ‘æœ‰3æ¡å‘½ï¼Œåœ¨å…³å¡ä¸­é—´" â†’ ä»·å€¼é«˜
   "æˆ‘åªæœ‰1æ¡å‘½ï¼Œå¿«æ­»äº†" â†’ ä»·å€¼ä½
   "æˆ‘å¿«åˆ°ç»ˆç‚¹äº†" â†’ ä»·å€¼å¾ˆé«˜ï¼

ğŸ¤– AIä¸­çš„ä»·å€¼å‡½æ•°ï¼š
   ä»·å€¼(çŠ¶æ€) = ä»è¿™ä¸ªçŠ¶æ€å¼€å§‹ï¼Œæœªæ¥èƒ½å¾—å¤šå°‘å¥–åŠ±

   ä¾‹å¦‚ï¼š
   ä»·å€¼("æ‰¾åˆ°äº†ç­”æ¡ˆ") = 1.0ï¼ˆéå¸¸å¥½ï¼‰
   ä»·å€¼("å®Œå…¨é”™è¯¯çš„æ–¹å‘") = -1.0ï¼ˆå¾ˆç³Ÿï¼‰
   ä»·å€¼("åˆšå¼€å§‹æ€è€ƒ") = 0.5ï¼ˆè¿˜å¯ä»¥ï¼‰
```

## 3.5 ä¼˜åŠ¿å‡½æ•°ï¼ˆAdvantage Functionï¼‰

**ä¼˜åŠ¿å‡½æ•°å›ç­”"è¿™ä¸ªåŠ¨ä½œæ¯”å¹³å‡å¥½å¤šå°‘"**

```
é€šä¿—ç†è§£ï¼š

æƒ³è±¡ä½ å‚åŠ è€ƒè¯•ï¼š
- ä½ å¾—äº†85åˆ†
- ç­å¹³å‡åˆ†æ˜¯75åˆ†
- é‚£ä½ çš„"ä¼˜åŠ¿"å°±æ˜¯ 85 - 75 = +10åˆ†

åœ¨RLä¸­ï¼š
ä¼˜åŠ¿(åŠ¨ä½œ) = è¿™ä¸ªåŠ¨ä½œçš„å¥–åŠ± - å¹³å‡å¥–åŠ±

å¦‚æœä¼˜åŠ¿ > 0ï¼šè¿™ä¸ªåŠ¨ä½œæ¯”å¹³å‡å¥½ï¼Œå¤šåšï¼
å¦‚æœä¼˜åŠ¿ < 0ï¼šè¿™ä¸ªåŠ¨ä½œæ¯”å¹³å‡å·®ï¼Œå°‘åšï¼
```

### GRPOä¸­çš„ä¼˜åŠ¿è®¡ç®—

```python
# æ¥è‡ªRAGENé¡¹ç›®

def compute_grpo_advantage(rewards, group_size):
    """
    è®¡ç®—GRPOä¼˜åŠ¿

    GRPOçš„ç‰¹ç‚¹ï¼šç”¨ç»„å†…ç›¸å¯¹ä¼˜åŠ¿ä»£æ›¿ç»å¯¹ä¼˜åŠ¿
    """
    advantages = []

    for i in range(len(rewards)):
        # æ‰¾åˆ°åŒç»„çš„å…¶ä»–ç­”æ¡ˆ
        group_start = (i // group_size) * group_size
        group_end = group_start + group_size
        group_rewards = rewards[group_start:group_end]

        # è®¡ç®—ç»„å†…å¹³å‡
        mean_reward = sum(group_rewards) / len(group_rewards)

        # è®¡ç®—æ ‡å‡†å·®
        import math
        std_reward = math.sqrt(
            sum((r - mean_reward) ** 2 for r in group_rewards) /
            len(group_rewards)
        )

        # ä¼˜åŠ¿ = (å½“å‰å¥–åŠ± - å¹³å‡) / æ ‡å‡†å·®
        advantage = (rewards[i] - mean_reward) / (std_reward + 1e-8)
        advantages.append(advantage)

    return advantages

# ä¾‹å­
rewards = [0.8, 0.5, 0.3, 0.9]  # 4ä¸ªç­”æ¡ˆ
# ç­”æ¡ˆ0æ¯”å¹³å‡å¥½ï¼Œä¼˜åŠ¿ä¸ºæ­£
# ç­”æ¡ˆ2æ¯”å¹³å‡å·®ï¼Œä¼˜åŠ¿ä¸ºè´Ÿ
```

---

# 4. æ‰‹æŠŠæ‰‹çœ‹ä»£ç 

## 4.1 ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒå¾ªç¯

è®©æˆ‘ä»¬ä»æœ€åŸºç¡€çš„ä»£ç çœ‹èµ·ï¼Œé€æ­¥ç†è§£RLè®­ç»ƒã€‚

### ç‰ˆæœ¬1ï¼šæœ€ç®€å•çš„å¾ªç¯ï¼ˆä¼ªä»£ç ï¼‰

```python
"""
è¿™æ˜¯ä¸€ä¸ªè¶…çº§ç®€åŒ–çš„RLè®­ç»ƒå¾ªç¯
å¸®åŠ©ç†è§£æ ¸å¿ƒæµç¨‹
"""

def train_agent_simple():
    # 1. åˆ›å»ºæ™ºèƒ½ä½“ï¼ˆç­–ç•¥ï¼‰
    agent = Agent(model="gpt-4")

    # 2. è®­ç»ƒå¾ªç¯
    for epoch in range(100):  # è®­ç»ƒ100è½®
        print(f"=== ç¬¬{epoch}è½®è®­ç»ƒ ===")

        # 3. ç”Ÿæˆé˜¶æ®µï¼šè®©æ™ºèƒ½ä½“å°è¯•
        for question in training_data:
            # ç”Ÿæˆå¤šä¸ªä¸åŒçš„ç­”æ¡ˆï¼ˆæ¢ç´¢ï¼‰
            answers = []
            for i in range(10):  # æ¯ä¸ªé—®é¢˜ç”Ÿæˆ10ä¸ªç­”æ¡ˆ
                answer = agent.generate(question)
                answers.append(answer)

            # 4. è¯„ä¼°é˜¶æ®µï¼šç»™æ¯ä¸ªç­”æ¡ˆæ‰“åˆ†
            scores = []
            for answer in answers:
                score = calculate_reward(question, answer)
                scores.append(score)

            # 5. å­¦ä¹ é˜¶æ®µï¼šä»å¥½ç­”æ¡ˆä¸­å­¦ä¹ 
            for answer, score in zip(answers, scores):
                if score > 0.5:  # åªä»å¥½ç­”æ¡ˆå­¦ä¹ 
                    agent.learn(question, answer, score)

        # 6. æµ‹è¯•ä¸€ä¸‹æ•ˆæœ
        test_score = evaluate(agent, test_data)
        print(f"ç¬¬{epoch}è½®ï¼Œæµ‹è¯•å¾—åˆ†ï¼š{test_score}")

def calculate_reward(question, answer):
    """è®¡ç®—å¥–åŠ±åˆ†æ•°"""
    # è¿™é‡Œå†™ä½ çš„å¥–åŠ±è§„åˆ™
    correct = check_answer(question, answer)
    if correct:
        return 1.0
    else:
        return 0.0
```

### ç‰ˆæœ¬2ï¼šåŠ å…¥PPOç®—æ³•

```python
"""
PPOï¼ˆProximal Policy Optimizationï¼‰çš„æ ¸å¿ƒæ€æƒ³
"""

def train_agent_ppo():
    agent = Agent(model="gpt-4")

    for epoch in range(100):
        # ========== ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆç­”æ¡ˆ ==========
        trajectories = []

        for question in training_data:
            for i in range(10):  # 10ä¸ªä¸åŒçš„ç­”æ¡ˆ
                answer = agent.generate(question)

                # è®°å½•è½¨è¿¹
                trajectories.append({
                    'question': question,
                    'answer': answer,
                    'log_prob': agent.get_log_prob(question, answer)  # è®°å½•æ¦‚ç‡
                })

        # ========== ç¬¬äºŒæ­¥ï¼šè®¡ç®—å¥–åŠ± ==========
        for traj in trajectories:
            traj['reward'] = calculate_reward(
                traj['question'],
                traj['answer']
            )

        # ========== ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—ä¼˜åŠ¿ ==========
        all_rewards = [t['reward'] for t in trajectories]

        for traj in trajectories:
            # ä¼˜åŠ¿ = è¿™ä¸ªç­”æ¡ˆçš„å¥–åŠ± - å¹³å‡å¥–åŠ±
            traj['advantage'] = traj['reward'] - (sum(all_rewards) / len(all_rewards))

        # ========== ç¬¬å››æ­¥ï¼šæ›´æ–°ç­–ç•¥ï¼ˆPPOæ ¸å¿ƒï¼‰==========
        for traj in trajectories:
            # é‡æ–°è®¡ç®—æ¦‚ç‡
            new_log_prob = agent.get_log_prob(
                traj['question'],
                traj['answer']
            )

            # PPOçš„å…³é”®ï¼šé™åˆ¶æ›´æ–°å¹…åº¦
            # æ¦‚ç‡æ¯” = æ–°æ¦‚ç‡ / æ—§æ¦‚ç‡
            ratio = math.exp(new_log_prob - traj['log_prob'])

            # å¦‚æœratioåœ¨[0.8, 1.2]ä¹‹é—´ï¼Œå°±æ­£å¸¸æ›´æ–°
            # å¦‚æœè¶…å‡ºèŒƒå›´ï¼Œå°±è£å‰ªï¼ˆä¸è¦æ›´æ–°å¤ªå¤šï¼‰
            clipped_ratio = max(0.8, min(1.2, ratio))

            # PPOæŸå¤±
            advantage = traj['advantage']

            # ä¸¤ç§é€‰æ‹©ï¼Œå–è¾ƒå°çš„ï¼ˆä¿å®ˆï¼‰
            loss1 = ratio * advantage
            loss2 = clipped_ratio * advantage
            loss = -min(loss1, loss2)  # è´Ÿå·æ˜¯å› ä¸ºè¦æœ€å¤§åŒ–

            # æ›´æ–°æ¨¡å‹
            agent.update(loss)
```

### ç‰ˆæœ¬3ï¼šçœŸå®é¡¹ç›®ä¸­çš„è®­ç»ƒå¾ªç¯

```python
"""
æ¥è‡ªRAGENé¡¹ç›®çš„å®é™…è®­ç»ƒä»£ç ï¼ˆç®€åŒ–ç‰ˆï¼‰
"""

class PPOTrainer:
    def __init__(self, actor_model, critic_model, config):
        self.actor = actor_model      # ç­–ç•¥æ¨¡å‹ï¼šç”Ÿæˆç­”æ¡ˆ
        self.critic = critic_model    # ä»·å€¼æ¨¡å‹ï¼šè¯„ä¼°çŠ¶æ€
        self.config = config

    def train(self, train_data, val_data):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""

        for epoch in range(self.config.total_epochs):
            print(f"=== Epoch {epoch} ===")

            # ====== é˜¶æ®µ1ï¼šRolloutï¼ˆç”Ÿæˆï¼‰ ======
            print("ç”Ÿæˆè½¨è¿¹...")
            trajectories = self._rollout_phase(train_data)

            # ====== é˜¶æ®µ2ï¼šè®¡ç®—å¥–åŠ±å’Œä¼˜åŠ¿ ======
            print("è®¡ç®—å¥–åŠ±...")
            self._compute_advantages(trajectories)

            # ====== é˜¶æ®µ3ï¼šæ›´æ–°æ¨¡å‹ ======
            print("æ›´æ–°æ¨¡å‹...")
            for batch in self._create_batches(trajectories):
                # æ›´æ–°Actor
                self._update_actor(batch)

                # æ›´æ–°Critic
                self._update_critic(batch)

            # ====== é˜¶æ®µ4ï¼šè¯„ä¼° ======
            if epoch % self.config.eval_every == 0:
                score = self.evaluate(val_data)
                print(f"è¯„ä¼°å¾—åˆ†ï¼š{score}")

    def _rollout_phase(self, data):
        """ç”Ÿæˆè½¨è¿¹"""
        trajectories = []

        for item in data:
            question = item['question']

            # ç”Ÿæˆå¤šä¸ªç­”æ¡ˆ
            for _ in range(self.config.num_rollouts):
                # Actorç”Ÿæˆç­”æ¡ˆ
                answer, log_prob = self.actor.generate(question)

                # Criticè¯„ä¼°çŠ¶æ€ä»·å€¼
                value = self.critic.evaluate(question)

                trajectories.append({
                    'question': question,
                    'answer': answer,
                    'log_prob': log_prob,
                    'value': value
                })

        return trajectories

    def _compute_advantages(self, trajectories):
        """è®¡ç®—ä¼˜åŠ¿ï¼ˆä½¿ç”¨GAEï¼‰"""

        for traj in trajectories:
            # 1. è·å–å¥–åŠ±
            reward = self._calculate_reward(traj)

            # 2. è®¡ç®—TDè¯¯å·®
            # TDè¯¯å·® = å¥–åŠ± + æŠ˜æ‰£å› å­ Ã— ä¸‹ä¸€çŠ¶æ€ä»·å€¼ - å½“å‰çŠ¶æ€ä»·å€¼
            td_error = (
                reward +
                self.config.gamma * traj['value'] -
                traj['value']
            )

            # 3. è®¡ç®—ä¼˜åŠ¿
            traj['advantage'] = td_error

    def _update_actor(self, batch):
        """æ›´æ–°Actorï¼ˆç­–ç•¥æ¨¡å‹ï¼‰"""

        for traj in batch:
            # é‡æ–°è®¡ç®—logæ¦‚ç‡
            new_log_prob = self.actor.get_log_prob(
                traj['question'],
                traj['answer']
            )

            # è®¡ç®—ratio
            ratio = math.exp(new_log_prob - traj['log_prob'])

            # PPOè£å‰ª
            clipped_ratio = max(
                1.0 - self.config.clip_ratio,
                min(1.0 + self.config.clip_ratio, ratio)
            )

            # è®¡ç®—æŸå¤±
            advantage = traj['advantage']
            policy_loss = -min(
                ratio * advantage,
                clipped_ratio * advantage
            )

            # æ›´æ–°
            self.actor.update(policy_loss)

    def _update_critic(self, batch):
        """æ›´æ–°Criticï¼ˆä»·å€¼æ¨¡å‹ï¼‰"""

        for traj in batch:
            # ç›®æ ‡ä»·å€¼ = å¥–åŠ± + æŠ˜æ‰£å› å­ Ã— ä¸‹ä¸€çŠ¶æ€ä»·å€¼
            target_value = (
                self._calculate_reward(traj) +
                self.config.gamma * traj['value']
            )

            # MSEæŸå¤±
            value_loss = (traj['value'] - target_value) ** 2

            # æ›´æ–°
            self.critic.update(value_loss)
```

---

# 5. å®æˆ˜æ¡ˆä¾‹

## 5.1 æ¡ˆä¾‹1ï¼šæ•™AIå­¦ä¼šä½¿ç”¨è®¡ç®—å™¨

è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªå®Œæ•´çš„ä¾‹å­ï¼šæ•™AIåœ¨åšæ•°å­¦é¢˜æ—¶å­¦ä¼šä½¿ç”¨Pythonä»£ç ã€‚

```python
"""
å®Œæ•´çš„Agent RLè®­ç»ƒç¤ºä¾‹
ç›®æ ‡ï¼šæ•™AIåœ¨åšæ•°å­¦é¢˜æ—¶ä½¿ç”¨Pythonä»£ç 
"""

# ==================== ç¬¬ä¸€æ­¥ï¼šå®šä¹‰ç¯å¢ƒ ====================

import re
import subprocess
import tempfile

class MathEnv:
    """
    æ•°å­¦é¢˜ç¯å¢ƒ

    çŠ¶æ€ï¼šæ•°å­¦é¢˜
    åŠ¨ä½œï¼šå†™ä»£ç æˆ–ä¸å†™ä»£ç 
    å¥–åŠ±ï¼šç­”æ¡ˆå¯¹ä¸å¯¹ + æ˜¯å¦ä½¿ç”¨äº†ä»£ç 
    """

    def __init__(self, question):
        self.question = question
        self.max_steps = 5

    def step(self, action):
        """
        æ‰§è¡ŒåŠ¨ä½œ

        actionæ ¼å¼ï¼š
        - ç›´æ¥ç»™ç­”æ¡ˆï¼š"ç­”æ¡ˆæ˜¯ï¼š42"
        - å†™ä»£ç ï¼š```python\nprint(123*456)\n```
        """

        # 1. æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ä»£ç 
        has_code = "```python" in action

        # 2. å¦‚æœæœ‰ä»£ç ï¼Œæ‰§è¡Œå®ƒ
        code_result = None
        if has_code:
            code = self._extract_code(action)
            code_result = self._execute_code(code)

        # 3. æå–ç­”æ¡ˆ
        answer = self._extract_answer(action, code_result)

        # 4. è®¡ç®—å¥–åŠ±
        reward = self._calculate_reward(answer, has_code)

        # 5. æ£€æŸ¥æ˜¯å¦ç»“æŸ
        done = self._is_correct(answer)

        return {
            'answer': answer,
            'code_result': code_result,
            'reward': reward,
            'done': done
        }

    def _extract_code(self, text):
        """æå–Pythonä»£ç """
        pattern = r'```python\n(.*?)```'
        match = re.search(pattern, text, re.DOTALL)
        if match:
            return match.group(1)
        return None

    def _execute_code(self, code):
        """æ‰§è¡ŒPythonä»£ç """
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(
                mode='w',
                suffix='.py',
                delete=False
            ) as f:
                f.write(code)
                temp_file = f.name

            # æ‰§è¡Œ
            result = subprocess.run(
                ['python', temp_file],
                capture_output=True,
                text=True,
                timeout=5
            )

            # æ¸…ç†
            import os
            os.unlink(temp_file)

            return result.stdout.strip()

        except Exception as e:
            return f"Error: {str(e)}"

    def _extract_answer(self, action, code_result):
        """æå–ç­”æ¡ˆ"""
        # å¦‚æœä»£ç æ‰§è¡ŒæˆåŠŸï¼Œç”¨ä»£ç ç»“æœ
        if code_result and not code_result.startswith("Error"):
            return code_result

        # å¦åˆ™ä»æ–‡æœ¬ä¸­æå–
        pattern = r'ç­”æ¡ˆæ˜¯[:ï¼š]\s*([0-9.]+)'
        match = re.search(pattern, action)
        if match:
            return match.group(1)

        return None

    def _calculate_reward(self, answer, has_code):
        """è®¡ç®—å¥–åŠ±"""
        reward = 0.0

        # 1. ç­”æ¡ˆæ­£ç¡®æ€§ï¼ˆ70%ï¼‰
        if self._is_correct(answer):
            reward += 0.7
        else:
            reward += 0.0

        # 2. ä½¿ç”¨äº†ä»£ç ï¼ˆ30%ï¼‰
        if has_code:
            reward += 0.3

        return reward

    def _is_correct(self, answer):
        """æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦æ­£ç¡®"""
        # è¿™é‡Œç®€åŒ–å¤„ç†
        # å®é™…ä¸­éœ€è¦å’Œæ ‡å‡†ç­”æ¡ˆæ¯”è¾ƒ
        correct_answer = self._get_correct_answer()
        return str(answer) == str(correct_answer)

    def _get_correct_answer(self):
        """è·å–æ­£ç¡®ç­”æ¡ˆ"""
        # ä»é—®é¢˜ä¸­æå–ï¼ˆç®€åŒ–ï¼‰
        if "123 Ã— 456" in self.question:
            return 123 * 456
        # å¯ä»¥æ·»åŠ æ›´å¤šè§„åˆ™
        return None


# ==================== ç¬¬äºŒæ­¥ï¼šå®šä¹‰Agent ====================

class MathAgent:
    """
    æ•°å­¦æ™ºèƒ½ä½“
    """

    def __init__(self, model):
        self.model = model
        self.temperature = 0.8  # é‡‡æ ·æ¸©åº¦

    def generate(self, question):
        """ç”Ÿæˆç­”æ¡ˆ"""
        # æ„å»ºæç¤º
        prompt = f"""é—®é¢˜ï¼š{question}

ä½ å¯ä»¥ï¼š
1. ç›´æ¥ç»™å‡ºç­”æ¡ˆ
2. ä½¿ç”¨Pythonä»£ç è®¡ç®—

è¯·å›ç­”ï¼š"""

        # ç”Ÿæˆ
        response = self.model.generate(
            prompt,
            temperature=self.temperature
        )

        return response

    def update(self, question, answer, reward):
        """ä»ç»éªŒä¸­å­¦ä¹ """
        # è¿™é‡Œåº”è¯¥æ˜¯å®é™…çš„æ¢¯åº¦æ›´æ–°
        # ç®€åŒ–ç‰ˆï¼šè®°å½•åˆ°ç»éªŒå›æ”¾ç¼“å†²åŒº
        print(f"å­¦ä¹ ï¼šé—®é¢˜={question}, ç­”æ¡ˆ={answer}, å¥–åŠ±={reward}")


# ==================== ç¬¬ä¸‰æ­¥ï¼šè®­ç»ƒå¾ªç¯ ====================

def train_math_agent():
    """è®­ç»ƒæ•°å­¦Agent"""

    # åˆå§‹åŒ–
    agent = MathAgent(model="gpt-4")
    questions = [
        "123 Ã— 456 = ?",
        "789 Ã— 234 = ?",
        "456 Ã— 789 = ?",
        # ... æ›´å¤šé¢˜ç›®
    ]

    # è®­ç»ƒå¾ªç¯
    for epoch in range(10):
        print(f"\n=== Epoch {epoch} ===")

        total_reward = 0

        for question in questions:
            # ç”Ÿæˆå¤šä¸ªç­”æ¡ˆ
            for _ in range(5):
                # Agentç”Ÿæˆç­”æ¡ˆ
                answer = agent.generate(question)

                # ç¯å¢ƒæ‰§è¡Œ
                env = MathEnv(question)
                result = env.step(answer)

                # å­¦ä¹ 
                agent.update(
                    question,
                    answer,
                    result['reward']
                )

                total_reward += result['reward']

                if result['done']:
                    print(f"âœ… ç­”å¯¹äº†ï¼")
                    break

        print(f"æ€»å¥–åŠ±ï¼š{total_reward}")


# è¿è¡Œ
if __name__ == "__main__":
    train_math_agent()
```

## 5.2 æ¡ˆä¾‹2ï¼šæ•™AIå­¦ä¼šæœç´¢

```python
"""
æ•™AIå­¦ä¼šåœ¨åšé¢˜æ—¶æœç´¢ç­”æ¡ˆ
"""

import requests
import json

class SearchEnv:
    """
    æœç´¢ç¯å¢ƒ

    çŠ¶æ€ï¼šé—®é¢˜
    åŠ¨ä½œï¼šæœç´¢æˆ–ç›´æ¥å›ç­”
    å¥–åŠ±ï¼šç­”æ¡ˆå¯¹ä¸å¯¹ + æ˜¯å¦æœ‰æ•ˆæœç´¢
    """

    def __init__(self, question, search_api_key):
        self.question = question
        self.search_api_key = search_api_key
        self.search_history = []

    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œ"""

        # 1. æ£€æŸ¥æ˜¯å¦æœç´¢
        has_search = "<search>" in action

        # 2. å¦‚æœæœç´¢ï¼Œæ‰§è¡Œæœç´¢
        search_result = None
        if has_search:
            query = self._extract_search_query(action)
            search_result = self._search(query)

        # 3. æå–ç­”æ¡ˆ
        answer = self._extract_answer(action, search_result)

        # 4. è®¡ç®—å¥–åŠ±
        reward = self._calculate_reward(
            answer,
            has_search,
            search_result
        )

        return {
            'answer': answer,
            'search_result': search_result,
            'reward': reward
        }

    def _extract_search_query(self, action):
        """æå–æœç´¢æŸ¥è¯¢"""
        pattern = r'<search>(.*?)</search>'
        match = re.search(pattern, action)
        if match:
            return match.group(1).strip()
        return None

    def _search(self, query):
        """æ‰§è¡Œæœç´¢"""
        # è°ƒç”¨æœç´¢APIï¼ˆä¾‹å¦‚Google Serperï¼‰
        url = "https://google.serper.dev/search"
        params = {
            "q": query,
            "key": self.search_api_key
        }

        response = requests.get(url, params=params)
        data = response.json()

        # è¿”å›æœç´¢ç»“æœ
        return data.get('organic', [])[:5]

    def _extract_answer(self, action, search_result):
        """æå–ç­”æ¡ˆ"""
        # å¦‚æœæœ‰æœç´¢ç»“æœï¼ŒåŸºäºæœç´¢ç»“æœå›ç­”
        if search_result:
            # è¿™é‡Œç®€åŒ–ï¼Œå®é™…åº”è¯¥è®©LLMåŸºäºæœç´¢ç»“æœç”Ÿæˆ
            pass

        # æå–<answer>æ ‡ç­¾
        pattern = r'<answer>(.*?)</answer>'
        match = re.search(pattern, action)
        if match:
            return match.group(1).strip()

        return None

    def _calculate_reward(self, answer, has_search, search_result):
        """è®¡ç®—å¥–åŠ±"""
        reward = 0.0

        # 1. ç­”æ¡ˆæ­£ç¡®æ€§ï¼ˆ60%ï¼‰
        if self._is_correct(answer):
            reward += 0.6

        # 2. æ˜¯å¦æœç´¢ï¼ˆ20%ï¼‰
        if has_search:
            reward += 0.2

        # 3. æœç´¢è´¨é‡ï¼ˆ20%ï¼‰
        if has_search and search_result:
            # æ£€æŸ¥æœç´¢ç»“æœæ˜¯å¦ç›¸å…³
            relevance = self._check_search_relevance(
                self.question,
                search_result
            )
            reward += relevance * 0.2

        return reward

    def _is_correct(self, answer):
        """æ£€æŸ¥ç­”æ¡ˆ"""
        # å’Œæ ‡å‡†ç­”æ¡ˆæ¯”è¾ƒ
        correct_answer = self._get_correct_answer()
        return str(answer) == str(correct_answer)

    def _check_search_relevance(self, question, search_results):
        """æ£€æŸ¥æœç´¢ç›¸å…³æ€§"""
        # ç®€åŒ–ï¼šæ£€æŸ¥æœç´¢ç»“æœæ˜¯å¦åŒ…å«é—®é¢˜å…³é”®è¯
        keywords = self._extract_keywords(question)

        relevant_count = 0
        for result in search_results:
            title = result.get('title', '')
            snippet = result.get('snippet', '')

            for keyword in keywords:
                if keyword.lower() in title.lower() or keyword.lower() in snippet.lower():
                    relevant_count += 1
                    break

        return relevant_count / len(search_results)

    def _extract_keywords(self, question):
        """æå–å…³é”®è¯"""
        # ç®€åŒ–ï¼šåˆ†è¯
        import jieba
        return jieba.lcut(question)


# ==================== è®­ç»ƒ ====================

def train_search_agent():
    """è®­ç»ƒæœç´¢Agent"""

    agent = MathAgent(model="gpt-4")
    search_api_key = "your_api_key"

    questions = [
        "ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ",
        "Pythonæ˜¯è°åˆ›é€ çš„ï¼Ÿ",
        # ...
    ]

    for epoch in range(10):
        print(f"\n=== Epoch {epoch} ===")

        for question in questions:
            # ç”Ÿæˆç­”æ¡ˆ
            answer = agent.generate(question)

            # ç¯å¢ƒ
            env = SearchEnv(question, search_api_key)
            result = env.step(answer)

            # å­¦ä¹ 
            agent.update(
                question,
                answer,
                result['reward']
            )


if __name__ == "__main__":
    train_search_agent()
```

---

# 6. å­¦ä¹ è·¯çº¿å›¾

## 6.1 ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€æ¦‚å¿µï¼ˆ1-2å‘¨ï¼‰

```
Week 1: ç†è§£Agentå’ŒRL
â”œâ”€â”€ é˜…è¯»æœ¬æ–‡æ¡£ï¼Œç†è§£æ ¸å¿ƒæ¦‚å¿µ
â”œâ”€â”€ è§‚çœ‹è§†é¢‘æ•™ç¨‹
â””â”€â”€ ç©ç®€å•çš„RLæ¸¸æˆï¼ˆå¦‚CartPoleï¼‰

Week 2: PythonåŸºç¡€
â”œâ”€â”€ å­¦ä¹ Pythonè¯­æ³•
â”œâ”€â”€ å­¦ä¹ PyTorchåŸºç¡€
â””â”€â”€ å®ç°ä¸€ä¸ªç®€å•çš„RLå¾ªç¯
```

## 6.2 ç¬¬äºŒé˜¶æ®µï¼šæ¡†æ¶å­¦ä¹ ï¼ˆ2-3å‘¨ï¼‰

```
Week 3-4: å­¦ä¹ é¡¹ç›®ä»£ç 
â”œâ”€â”€ ä»RAGENå¼€å§‹ï¼ˆç¯å¢ƒç®€å•ï¼‰
â”œâ”€â”€ ç†è§£MDPã€ç­–ç•¥ã€å¥–åŠ±
â””â”€â”€ è¿è¡Œç¤ºä¾‹ä»£ç 

Week 5: veRLæ¡†æ¶
â”œâ”€â”€ å­¦ä¹ veRLæ–‡æ¡£
â”œâ”€â”€ ç†è§£PPOã€GRPOç®—æ³•
â””â”€â”€ è¿è¡Œå®Œæ•´è®­ç»ƒ
```

## 6.3 ç¬¬ä¸‰é˜¶æ®µï¼šé¡¹ç›®å®è·µï¼ˆ3-4å‘¨ï¼‰

```
Week 6-7: ç®€å•é¡¹ç›®
â”œâ”€â”€ RAGENï¼ˆSokobanç¯å¢ƒï¼‰
â”œâ”€â”€ ReToolï¼ˆä»£ç æ‰§è¡Œï¼‰
â””â”€â”€ ç†è§£å¥–åŠ±è®¾è®¡

Week 8-9: å¤æ‚é¡¹ç›®
â”œâ”€â”€ R1-Searcherï¼ˆæœç´¢é›†æˆï¼‰
â”œâ”€â”€ DeepResearcherï¼ˆçœŸå®ç¯å¢ƒï¼‰
â””â”€â”€ å°è¯•æ”¹è¿›
```

## 6.4 ç¬¬å››é˜¶æ®µï¼šæ·±å…¥ç ”ç©¶ï¼ˆæŒç»­ï¼‰

```
Week 10+: è®ºæ–‡å’Œæ”¹è¿›
â”œâ”€â”€ é˜…è¯»åŸå§‹è®ºæ–‡
â”œâ”€â”€ å¤ç°å®éªŒç»“æœ
â”œâ”€â”€ è®¾è®¡æ–°çš„å¥–åŠ±å‡½æ•°
â””â”€â”€ å‘è¡¨è‡ªå·±çš„ç ”ç©¶
```

---

## 7. å¸¸è§é—®é¢˜è§£ç­”

### Q1: Agent RLå’Œæ™®é€šChatGPTæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

```
æ™®é€šChatGPTï¼š
- ä½ é—®ï¼Œå®ƒç­”
- æ¯æ¬¡å›ç­”æ˜¯ç‹¬ç«‹çš„
- ä¸ä¼šä»åé¦ˆä¸­å­¦ä¹ 

Agent RLï¼š
- å¯ä»¥ä¸»åŠ¨è¡ŒåŠ¨ï¼ˆæœç´¢ã€è®¡ç®—ï¼‰
- å¯ä»¥å¤šè½®äº¤äº’
- ä¼šä»å¥–åŠ±/æƒ©ç½šä¸­å­¦ä¹ 
- ç›®æ ‡æ˜¯æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±
```

### Q2: ä¸ºä»€ä¹ˆè¦ç”Ÿæˆå¤šä¸ªç­”æ¡ˆï¼Ÿ

```
åŸå› 1: æ¢ç´¢
å¦‚æœåªç”Ÿæˆä¸€ä¸ªç­”æ¡ˆï¼Œå¯èƒ½è¿æ°”å¥½æˆ–ä¸å¥½
ç”Ÿæˆå¤šä¸ªç­”æ¡ˆï¼Œå¯ä»¥çœ‹åˆ°"ä¸åŒå°è¯•"

åŸå› 2: ä¼˜åŠ¿è®¡ç®—
GRPOéœ€è¦å¤šä¸ªç­”æ¡ˆæ¥è®¡ç®—"ç›¸å¯¹ä¼˜åŠ¿"
ç­”æ¡ˆAæ¯”å…¶ä»–ç­”æ¡ˆå¥½ â†’ æ­£ä¼˜åŠ¿
ç­”æ¡ˆBæ¯”å…¶ä»–ç­”æ¡ˆå·® â†’ è´Ÿä¼˜åŠ¿

ä¾‹å­ï¼š
é—®é¢˜: "1+1=?"
ç”Ÿæˆçš„10ä¸ªç­”æ¡ˆï¼š
[2, 2, 2, 2, 3, 2, 2, 1, 2, 2]
å¹³å‡ â‰ˆ 1.8

ç­”æ¡ˆ"1"çš„ä¼˜åŠ¿ = 1 - 1.8 = -0.8ï¼ˆå·®ï¼‰
ç­”æ¡ˆ"3"çš„ä¼˜åŠ¿ = 3 - 1.8 = +1.2ï¼ˆå¥½ï¼‰
ç­”æ¡ˆ"2"çš„ä¼˜åŠ¿ â‰ˆ 2 - 1.8 = +0.2ï¼ˆå¥½ï¼‰
```

### Q3: ä»€ä¹ˆæ˜¯"Rollout"ï¼Ÿ

```
Rolloutå°±æ˜¯"è®©æ¨¡å‹è¯•ç€åšä¸€é"

ç±»æ¯”ï¼š
ä½ åœ¨å­¦éª‘è‡ªè¡Œè½¦
Rolloutå°±æ˜¯"éª‘ä¸€æ¬¡"çš„è¿‡ç¨‹

åœ¨ä»£ç ä¸­ï¼š
def rollout(agent, question, max_steps=10):
    """æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„rollout"""

    history = []
    for step in range(max_steps):
        # æ¨¡å‹ç”ŸæˆåŠ¨ä½œ
        action = agent.generate(question, history)

        # æ‰§è¡ŒåŠ¨ä½œï¼Œå¾—åˆ°åé¦ˆ
        feedback = environment.step(action)

        # è®°å½•
        history.append({
            'action': action,
            'feedback': feedback
        })

        # æ£€æŸ¥æ˜¯å¦ç»“æŸ
        if feedback['done']:
            break

    return history
```

### Q4: ä»€ä¹ˆæ˜¯"Episode"ï¼ˆå›åˆï¼‰ï¼Ÿ

```
Episode = ä»å¼€å§‹åˆ°ç»“æŸçš„ä¸€æ¬¡å®Œæ•´ç»å†

ä¾‹å­1: æ‰“æ¸¸æˆ
Episode = ä»å¼€å§‹æ¸¸æˆåˆ°é€šå…³æˆ–å¤±è´¥

ä¾‹å­2: æ•°å­¦é¢˜
Episode = ä»çœ‹åˆ°é¢˜ç›®åˆ°ç»™å‡ºæœ€ç»ˆç­”æ¡ˆ

ä¾‹å­3: å¯¹è¯
Episode = ä»ç”¨æˆ·æé—®åˆ°é—®é¢˜è§£å†³

åœ¨è®­ç»ƒä¸­ï¼š
for episode in range(1000):
    state = env.reset()

    while not done:
        action = agent.act(state)
        next_state, reward, done = env.step(action)
        agent.learn(state, action, reward, next_state)
        state = next_state
```

---

## 8. æ ¸å¿ƒä»£ç è¯¦è§£

### 8.1 æ¥è‡ªDeepResearcherçš„Handler

è¿™æ˜¯æœ€çœŸå®çš„Agent RLä»£ç ï¼Œè®©æˆ‘ä»¬ä»”ç»†çœ‹çœ‹ï¼š

```python
"""
DeepResearcherçš„Handlerä»£ç è¯¦è§£
è¿™ä¸ªç±»è´Ÿè´£åè°ƒæœç´¢å’Œé˜…è¯»æ™ºèƒ½ä½“
"""

class Handler:
    """
    Handleræ˜¯æ•´ä¸ªç³»ç»Ÿçš„"å¤§è„‘"

    èŒè´£ï¼š
    1. æ¥æ”¶æ¨¡å‹çš„å·¥å…·è°ƒç”¨è¯·æ±‚
    2. æ‰§è¡Œæœç´¢
    3. é˜…è¯»ç½‘é¡µ
    4. è¿”å›ç»“æœç»™æ¨¡å‹
    """

    def __init__(self, agent_config, client):
        # ä¸¤ä¸ªæ™ºèƒ½ä½“
        self.web_search_agent = WebSearchAgent(config, client)
        self.reading_agent = ReadingAgent(config, client)

        # ç¼“å­˜
        self.api_result_dict = {}  # æœç´¢ç»“æœç¼“å­˜
        self.id_to_context = {}      # å¯¹è¯ä¸Šä¸‹æ–‡

    def handle_execution_api(self, query_contents):
        """
        ä¸»å¤„ç†å‡½æ•°

        query_contentsç¤ºä¾‹ï¼š
        [
            {
                "idx": 0,
                "question": "é‡å­è®¡ç®—æœºæœ€æ–°è¿›å±•ï¼Ÿ",
                "tool_call": {
                    "name": "web_search",
                    "arguments": {
                        "query": ["quantum computing 2024"]
                    }
                }
            }
        ]
        """

        # ====== ç¬¬ä¸€é˜¶æ®µï¼šæœç´¢ ======
        print("å¼€å§‹å¤„ç†æœç´¢è¯·æ±‚...")

        # æ£€æŸ¥ç¼“å­˜ï¼ˆ7å¤©æœ‰æ•ˆæœŸï¼‰
        cache_hit = 0
        total_search = 0

        for query_content in query_contents:
            if query_content['tool_call']['name'] != 'web_search':
                continue

            query_list = query_content['tool_call']['arguments']['query']
            total_search += len(query_list)

            for query in query_list:
                # æ£€æŸ¥ç¼“å­˜
                if query in self.api_result_dict:
                    timestamp = self.api_result_dict[query]['timestamp']
                    # 7å¤©å†…æœ‰æ•ˆ
                    if time.time() - timestamp <= 7 * 24 * 3600:
                        cache_hit += 1
                        continue

                # éœ€è¦æœç´¢
                self.api_result_dict[query] = {
                    "timestamp": time.time(),
                    "organic": []  # ç¨åå¡«å……
                }

        print(f"ç¼“å­˜å‘½ä¸­ç‡ï¼š{cache_hit}/{total_search}")

        # å¹¶å‘æ‰§è¡Œæœç´¢ï¼ˆ1000ä¸ªworkerï¼ï¼‰
        with ThreadPoolExecutor(max_workers=1000) as executor:
            futures = []
            for query in self.api_result_dict:
                if self.api_result_dict[query]['organic']:
                    continue  # å·²æœ‰ç¼“å­˜

                future = executor.submit(
                    self._search_and_add_to_dict,
                    query,
                    self.api_result_dict
                )
                futures.append(future)

            # ç­‰å¾…æ‰€æœ‰æœç´¢å®Œæˆ
            for future in futures:
                future.result()

        # ä¿å­˜ç¼“å­˜
        with open(self.query_save_path, 'w') as f:
            json.dump(self.api_result_dict, f)

        # ====== ç¬¬äºŒé˜¶æ®µï¼šå¤„ç†æ¯ä¸ªæŸ¥è¯¢ ======
        print("å¼€å§‹å¤„ç†æŸ¥è¯¢ç»“æœ...")

        with ThreadPoolExecutor(max_workers=100) as executor:
            futures = []
            for query_content in query_contents:
                future = executor.submit(
                    self.handle_single_query,
                    query_content,
                    self.api_result_dict
                )
                futures.append(future)

            # æ”¶é›†ç»“æœ
            for i, future in enumerate(futures):
                query_contents[i]["content"] = future.result()

        print("å¤„ç†å®Œæˆ")
        return query_contents

    def handle_single_query(self, query_content, api_result_dict):
        """
        å¤„ç†å•ä¸ªæŸ¥è¯¢
        """
        idx = query_content["idx"]
        question = query_content["question"]
        tool_call = query_content["tool_call"]

        func_name = tool_call["name"]
        arguments = tool_call["arguments"]

        if func_name == "web_search":
            # ====== æ‰§è¡Œæœç´¢ ======
            search_query_list = arguments["query"]

            # æ‰¹é‡æœç´¢
            web_page_info_list_batch = self.web_search_agent.search_web_batch(
                user_query=question,
                search_query_list=search_query_list,
                api_result_dict=api_result_dict
            )

            # æ ¼å¼åŒ–ç»“æœ
            content = []
            for search_query, web_page_info_list in zip(
                search_query_list,
                web_page_info_list_batch
            ):
                ret_list = []
                for web_page_info in web_page_info_list:
                    ret_list.append({
                        "title": web_page_info.title,
                        "url": web_page_info.url,
                        "quick_summary": web_page_info.quick_summary
                    })

                content.append({
                    "search_query": search_query,
                    "web_page_info_list": ret_list
                })

            return content

        elif func_name == "browse_webpage":
            # ====== é˜…è¯»ç½‘é¡µ ======
            url_list = arguments["url_list"]

            # æ‰¹é‡é˜…è¯»
            read_webpage_list = self.reading_agent.read_batch(
                user_query=question,
                search_result_info_list=self.id_to_context[idx][-1].search_result_info_list,
                url_list=url_list,
                web_search_agent=self.web_search_agent
            )

            # æ ¼å¼åŒ–ç»“æœ
            content = []
            for read_webpage in read_webpage_list:
                information = []
                for page_read_info in read_webpage.page_read_info_list:
                    information.append({
                        "page_number": page_read_info.page_number,
                        "page_summary": page_read_info.page_summary
                    })

                content.append({
                    "url": read_webpage.url,
                    "information": information
                })

            return content
```

### 8.2 æ¥è‡ªR1-Searcherçš„å¥–åŠ±è®¡ç®—

```python
"""
å¥–åŠ±å‡½æ•°è®¾è®¡è¯¦è§£

è¿™æ˜¯R1-Searcherçš„æ ¸å¿ƒï¼šå¦‚ä½•ç»™ç­”æ¡ˆæ‰“åˆ†
"""

class RewardCalculator:
    """
    å¥–åŠ±è®¡ç®—å™¨

    æ ¸å¿ƒæ€æƒ³ï¼šç”¨å¥–åŠ±å¼•å¯¼æ¨¡å‹å­¦ä¼šæœç´¢
    """

    def get_reward(self, queries):
        """
        è®¡ç®—å¥–åŠ±

        queries: æ¨¡å‹ç”Ÿæˆçš„å¤šä¸ªç­”æ¡ˆ
        è¿”å›: æ¯ä¸ªç­”æ¡ˆçš„å¥–åŠ±åˆ†æ•°
        """
        scores = []

        for i, query in enumerate(queries):
            # ====== æå–é—®é¢˜å’Œç­”æ¡ˆ ======
            question, solution = self.parse_query(query)
            pred_answer = self.extract_answer(solution)
            true_answer = self.get_ground_truth(question)

            # ====== è®¡ç®—F1åˆ†æ•° ======
            f1_score = self.calculate_f1(pred_answer, true_answer)
            scores.append(float(f1_score))

            # ====== æ£€æŸ¥æ ¼å¼ ======
            # æ ¼å¼æ£€æŸ¥ï¼šå¿…é¡»æœ‰ç‰¹å®šæ ‡ç­¾
            if not self.has_valid_format(solution):
                scores[i] = 0.0  # æ ¼å¼é”™è¯¯ï¼Œ0åˆ†ï¼
                continue

            # ====== æ£€æŸ¥æ ‡ç­¾é…å¯¹ ======
            if not self.check_tag_pairs(solution):
                scores[i] -= 2.0  # æ ‡ç­¾ä¸é…å¯¹ï¼Œé‡ç½šï¼
                continue

            # ====== æ£€æŸ¥æ˜¯å¦å®Œæˆ ======
            if self.is_complete(solution):
                # å®Œæˆä¸”æœ‰æ­£ç¡®æ ¼å¼ï¼Œå¥–åŠ±æ›´é«˜
                scores[i] += 0.5

        return scores

    def calculate_f1(self, pred, truth):
        """
        è®¡ç®—F1åˆ†æ•°

        F1 = 2 Ã— (ç²¾ç¡®ç‡ Ã— å¬å›ç‡) / (ç²¾ç¡®ç‡ + å¬å›ç‡)
        """
        # æ ‡å‡†åŒ–
        pred = self.normalize(pred)
        truth = self.normalize(truth)

        # åˆ†è¯
        pred_tokens = pred.split()
        truth_tokens = truth.split()

        # è®¡ç®—å…±åŒtokenæ•°
        common = set(pred_tokens) & set(truth_tokens)
        num_common = len(common)

        if num_common == 0:
            return 0.0

        # ç²¾ç¡®ç‡ = å…±åŒ/é¢„æµ‹
        precision = num_common / len(pred_tokens)

        # å¬å›ç‡ = å…±åŒ/çœŸå®
        recall = num_common / len(truth_tokens)

        # F1åˆ†æ•°
        f1 = (2 * precision * recall) / (precision + recall)

        return f1

    def has_valid_format(self, solution):
        """æ£€æŸ¥æ ¼å¼"""
        required_tags = [
            "<thinking>",
            "<search>",
            "<documents>",
            "<answer>"
        ]

        for tag in required_tags:
            if tag not in solution:
                return False

        return True

    def check_tag_pairs(self, solution):
        """æ£€æŸ¥æ ‡ç­¾é…å¯¹"""
        tags = ["<search>", "<documents>", "<answer>"]

        for tag in tags:
            open_tag = tag
            close_tag = tag.replace("<", "</").replace(">", ">")

            count_open = solution.count(open_tag)
            count_close = solution.count(close_tag)

            if count_open != count_close:
                return False  # ä¸é…å¯¹ï¼

        return True
```

---

## æ€»ç»“

### Agent RLçš„æ ¸å¿ƒæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚   1. è§‚å¯Ÿ        2. å†³ç­–       3. è¡ŒåŠ¨       4. åé¦ˆ        â”‚
â”‚   çŠ¶æ€ â”€â”€â”€â”€â–º ç­–ç•¥ â”€â”€â”€â”€â–º åŠ¨ä½œ â”€â”€â”€â”€â–º å¥–åŠ±               â”‚
â”‚      â†‘                                          â”‚            â”‚
â”‚      â”‚                                          â†“            â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ›´æ–°ç­–ç•¥ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚   å°±åƒå­¦ä¹ éª‘è‡ªè¡Œè½¦ï¼š                                     â”‚
â”‚   - è§‚å¯Ÿï¼šè·¯å†µã€è‡ªè¡Œè½¦çŠ¶æ€                                â”‚
â”‚   - å†³ç­–ï¼šè¦ä¸è¦åŠ é€Ÿã€è½¬å‘                                 â”‚
â”‚   - è¡ŒåŠ¨ï¼šè½¬åŠ¨è½¦æŠŠã€è¸©è¸æ¿                                 â”‚
â”‚   - åé¦ˆï¼šéª‘ç¨³äº†è¿˜æ˜¯æ‘”äº†                                 â”‚
â”‚   - æ›´æ–°ï¼šè°ƒæ•´éª‘è½¦ç­–ç•¥                                     â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å­¦ä¹ å»ºè®®

1. **ä»ç®€å•å¼€å§‹**
   - å…ˆç†è§£æ¦‚å¿µ
   - å†çœ‹ä»£ç 
   - æœ€ååŠ¨æ‰‹å®è·µ

2. **å¾ªåºæ¸è¿›**
   - ä»RAGENå¼€å§‹ï¼ˆæœ€ç®€å•ï¼‰
   - é€æ­¥å­¦ä¹ å…¶ä»–é¡¹ç›®
   - ä¸è¦æ€¥äºæ±‚æˆ

3. **å¤šåšå®éªŒ**
   - ä¿®æ”¹å‚æ•°
   - è§‚å¯Ÿæ•ˆæœ
   - ç†è§£åŸå› 

4. **å–„ç”¨èµ„æº**
   - é¡¹ç›®README
   - è®ºæ–‡
   - ç¤¾åŒºè®¨è®º

ç¥ä½ å­¦ä¹ é¡ºåˆ©ï¼ğŸš€
