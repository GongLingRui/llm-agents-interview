https://lucumr.pocoo.org/2025/11/22/llm-apis/


# 《LLM APIs are a Synchronization Problem》网页总结
## 一、核心观点
作者Armin Ronacher认为，当前通过API使用大语言模型（LLM）时，所依赖的API抽象层并非底层技术的最佳适配，LLM API的本质问题实则是**分布式状态同步问题**，而非单纯的消息交互问题。


## 二、LLM核心工作原理与状态本质
1. **基础运行逻辑**：LLM先将文本 token 化（转为数字），再通过GPU上的矩阵乘法与注意力层运算，基于固定权重生成激活值并预测下一个token；若排除“温度”（随机化）因素，理论上可视为高确定性系统。
2. **token无本质差异**：对核心模型而言，“用户文本”与“助手文本”无本质区别，均为token；角色（系统、用户、助手、工具）区分仅通过提示模板注入的特殊token和格式实现。
3. **两类关键状态**：
    - 可见状态：对话历史对应的token（可通过重放恢复文本内容）。
    - 隐藏状态：模型在GPU上的“工作状态”（如注意力键值缓存KV cache），无法仅通过重放token恢复，需专门维护。


## 三、现有主流LLM API的问题
### （一）Completion API（完成式API，如OpenAI、Anthropic）
1. **抽象层不一致**：用户与GPU对对话历史的理解处于不同抽象层，用户无法感知API注入的额外token（如JSON消息转译token、工具定义token、缓存点信息等）。
2. **隐藏关键信息**：部分LLM提供商为防止用户复用数据训练自有模型，会隐藏推理token、搜索结果及注入逻辑，仅返回加密数据块，需用户回传才能继续对话，增加状态同步难度。
3. **数据传输与成本问题**：每轮对话需重发完整提示历史，单请求大小随轮次线性增长，长期对话总数据量呈**二次方增长**，既推高成本，也使服务器注意力计算成本随序列长度二次方增长。

### （二）Responses API（响应式API，OpenAI推出）
1. **状态同步能力弱**：虽在服务器端维护对话历史（需开启保存状态标识），但API仅提供有限同步功能，用户无法明确对话可持续时长、状态分歧/损坏后的处理方式，且存在网络分区（如一方接收状态更新另一方未接收）、状态无法恢复等问题。
2. **使用门槛高**：当前暴露的API设计复杂，较难上手；对提供商有利（可隐藏更多后台状态），但对用户不友好。


## 四、LLM API的核心痛点：隐藏状态同步混乱
1. **隐藏状态无标准**：所有LLM提供商都会在后台注入隐藏上下文（提示模板、角色标记、系统/工具定义、提供商侧工具输出等），但此类隐藏状态的表示、同步方式无统一标准，各厂商实现不兼容。
2. **中间层加剧复杂**：通过OpenRouter（中间代理）或Vercel AI SDK（开发工具包）统一API时，虽能掩盖部分厂商差异，但无法解决各提供商“私有隐藏状态”的同步问题，成为API统一的最大障碍。


## 五、解决思路：借鉴与未来方向
### （一）借鉴“本地优先”（Local First）生态
“本地优先”领域已解决分布式状态同步难题（如跨不信任客户端/服务器的同步、离线恢复、分支合并等），其核心经验可迁移至LLM API：
- 明确区分“标准状态”（如对话token日志）、“派生状态”（如可 checkpoint 并恢复的KV cache）、“传输机制”，填补当前LLM API的设计空白。
- 关键映射：KV cache对应“可 checkpoint 的派生状态”，对话历史对应“可增量同步的追加式日志”，提供商隐藏上下文对应“带隐藏字段的可复制文档”。
- 额外要求：需支持“远程状态丢失时从初始状态完整重放”，而当前Responses API不具备此能力。

### （二）未来统一API的设计原则
1. **摒弃“消息驱动”思维**：当前基于消息交互的API抽象层可能不符合LLM长期发展需求，应从模型实际运行机制（而非历史继承的表面约定）出发设计API。
2. **明确核心要素**：统一标准需涵盖“隐藏状态处理”“同步边界”“重放语义”“故障模式”等真实问题，避免仓促固化现有API的缺陷。
3. **警惕标准化风险**：避免急于将当前不完善的API抽象层形式化，防止锁定其弱点，需先探索更贴合LLM本质的抽象方案。
