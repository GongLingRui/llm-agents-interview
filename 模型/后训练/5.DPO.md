<img width="486" height="114" alt="image" src="https://github.com/user-attachments/assets/4a801aa3-c294-4b9e-8132-e82c9cba8cce" />


### 4. DPO相关问题
#### 4.1 DPO针对RLHF做了哪些改进
通过RLHF与DPO的算法流程对比，体现DPO的改进：
- **RLHF算法流程**：包含**奖励模型（reward model）**和**策略模型（policy model/actor model）**，基于偏好数据+强化学习不断迭代优化策略模型。
- **DPO算法流程**：不包含奖励模型和强化学习过程，直接通过偏好数据微调，将强化学习过程转换为SFT过程，训练更简单、高效（核心改进体现于损失函数）。


（注：论文中`chosen`对应下标w（win），`rejected`对应下标l（lose））


#### 4.1.1 通过PPO进一步理解RLHF的过程

<img width="475" height="141" alt="image" src="https://github.com/user-attachments/assets/a99a3a82-fd72-4b85-8064-0a8edf2e3c10" />




### RLHF（基于PPO）的特点与模型组成
#### 1. RLHF的整体特点
RLHF常以PPO为基础算法，包含4个模型；训练中需对policy model采样，因此**稳定性、效率、效果不易控制**。


#### 2. 4个核心模型
- **Policy模型（又称Actor）**
  - 功能：输入上文，输出下一个token的概率分布（即模型的“行为”）；
  - 定位：待训练的模型，最终得到的目标模型；通常以SFT训练后的模型初始化。

- **Value模型（又称Critic）**
  - 功能：预估当前模型回复的**总收益**（不仅看当前token质量，还需衡量其对后续文本生成的影响）；
  - 定位：需训练，作用是估计状态/状态动作对的长期价值（状态值函数/动作值函数）。

- **Reward模型**
  - 功能：用偏好数据预先训练，对Policy模型的预测**打分**，评估当前输出的**即时收益**；
  - 定位：提供每个状态/状态动作对的即时奖励信号。

- **Reference模型**
  - 功能：与Policy模型结构相同，但训练中不优化更新；用于维持模型训练表现，防止更新时出现过大偏差。


