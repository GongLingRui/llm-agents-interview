这篇论文名为 **《ReAct: Synergizing Reasoning and Acting in Language Models》**（ReAct：在语言模型中协同推理与行动），发表于 **ICLR 2023** 。它提出了一种全新的提示（Prompting）范式，旨在解决大语言模型（LLM）在复杂任务中“空谈而不实践”或“盲目行动”的问题。

以下是对该论文核心要点的详细讲解：

### 1. 核心动机：为什么要结合“推理”与“行动”？

作者观察到，人类智能的一个独特特征是能将**面向任务的行动**与**语言推理**（即“内心独白”）无缝结合 。

* 
**Chain-of-Thought (CoT) 的局限：** 虽然 CoT 提升了推理能力，但它是一个“静态黑盒”，模型只靠内部知识推理，无法与外部世界交互。这容易导致**事实幻觉（Hallucination）**和**错误累积** 。


* 
**Act-Only（仅行动）的局限：** 像 WebGPT 这样只关注预测动作的模型，缺乏对高层目标的抽象推理和工作记忆的维护，难以处理复杂的任务逻辑 。



**ReAct 的核心思想：** 将语言模型的目标空间扩展，允许它交替生成**推理轨迹（Thoughts）**和**具体行动（Actions）** 。

* 
**Reason to Act（推理指导行动）：** 推理轨迹帮助模型诱导、跟踪并更新行动计划，同时处理异常情况 。


* 
**Act to Reason（行动支持推理）：** 通过与外部环境（如维基百科、外部数据库）交互，获取新信息，进而修正推理 。



### 2. 方法论：ReAct 的运行方式

在 ReAct 范式下，智能体（Agent）在每个时间步的操作包含：

1. 
**Thought（思考）：** 自由格式的语言，用于分解任务、提取关键观察、调整策略或运用常识 。


2. 
**Action（行动）：** 调用特定 API 或环境指令（如 `search[entity]`、`click[button]`） 。


3. 
**Observation（观察）：** 外部环境返回的信息（如网页内容、物体状态），这些信息会被加入到上下文（Context）中，辅助下一步的推理 。



### 3. 实验任务与表现

论文在两类极具挑战性的任务上测试了 ReAct（主要基于 PaLM-540B 模型）：

#### A. 知识密集型推理任务 (HotpotQA & FEVER)

* 
**设置：** 模型通过简单的维基百科 API 进行多步搜索来回答问题 。


* 
**表现：** ReAct 在抑制幻觉方面表现优异。在 HotpotQA（多跳问答）中，虽然略逊于 CoT，但在 FEVER（事实核实）上超过了 CoT 。


* 
**组合策略：** 作者发现 **ReAct + CoT-SC**（自一致性）效果最好。即当模型通过内部知识（CoT）无法得出一致答案时，自动切换到 ReAct 去外部找证据 。



#### B. 交互式决策任务 (ALFWorld & WebShop)

* 
**设置：** 在文本模拟环境（ALFWorld）中移动物体，或在模拟购物网站（WebShop）中搜索并购买商品 。


* 
**表现：** 仅需 **1-2 个示例（Few-shot）**，ReAct 的成功率就比传统的模仿学习（IL）或强化学习（RL）方法高出 **34% 和 10%**，而后者通常需要成千上万个样本的训练 。



### 4. 优势与局限性

优势 ：

1. **可解释性 (Interpretability)：** 推理轨迹让模型的操作过程透明，人类可以看清它是基于什么证据做出的决定。
2. **可诊断性 (Diagnosability)：** 当模型出错时，可以轻易通过日志判断是推理错了、行动错了，还是外部返回了无效信息。
3. **可控性 (Controllability)：** 人类可以通过修改模型的“Thought”实时干预其行动方向。

局限性 ：

1. 
**搜索瓶颈：** 如果外部 API 返回了无用信息（Non-informative search），模型往往很难从中恢复推理轨迹 。


2. 
**多步循环：** 模型有时会陷入重复生成相同动作和想法的“死循环” 。


3. 
**上下文长度限制：** 复杂的交互历史很容易超出大模型的 Token 限制 。



### 5. 总结 (Conclusion)

ReAct 证明了通过简单的 **推理（Reasoning）与行动（Acting）的协同**，大语言模型即便不经过大规模微调，也能在需要多步搜索和环境互动的复杂任务中展现出极强的鲁棒性和逻辑性 。它目前已成为构建主流 AI Agent（如 AutoGPT, LangChain 等）的核心逻辑框架。

这篇论文确实涉及到了**微调（Fine-tuning）**的内容。虽然 ReAct 的核心在于通过**少样本提示（Few-shot prompting）**让大型冻结模型（如 PaLM-540B）协同推理与行动，但作者也探讨了如何通过微调来提升较小规模模型的性能 。

以下是关于论文中微调部分的详细讲解：

### 1. 为什么要进行微调？

作者指出，尽管大模型在 Few-shot 下表现优异，但存在一些局限性 ：

* 
**上下文长度限制：** 复杂的任务需要大量的推理和行动轨迹，这些演示（Demonstrations）很容易超出模型的输入长度限制 。


* 
**小模型能力不足：** 较小的模型（如 8B 或 62B 参数的模型）仅靠提示词很难同时学会推理和行动的复杂逻辑 。



### 2. 微调的具体操作流程 (Self-Correction / Bootstrapping)

论文采用了一种类似于 **自我提升（Bootstrapping）** 的微调方法 ：

1. 
**数据生成：** 使用具有 540B 参数的大模型（ReAct 提示模式）在训练集上运行，生成大量的解决问题的轨迹（包含 Thought, Action, Observation） 。


2. 
**筛选正确轨迹：** 从这些生成的轨迹中，仅保留那些最终回答正确（或任务达成）的样本 。在实验中，作者筛选出了约 **3,000 条** 正确的轨迹作为微调数据 。


3. 
**训练模型：** 使用这些筛选出的“高质量轨迹”作为监督数据，对较小的模型（如 **PaLM-8B** 和 **PaLM-62B**）进行微调 。


4. 
**输入与输出：** 训练时，模型被教授在给定输入问题或主张的情况下，按顺序解码出完整的轨迹（所有的思考、行动和对观察的预期处理） 。



### 3. 微调的效果与结论

通过实验，作者得出了几个重要结论：

* 
**性能飞跃：** 经过微调后的较小模型表现大幅提升 。例如，**微调后的 8B 模型性能甚至超过了未经微调（仅通过提示）的 62B 模型**；而微调后的 62B 模型性能超过了提示模式下的 540B 大模型 。


* 
**泛化性更强：** 相比于只微调答案（Standard）或只微调推理过程（CoT），微调 ReAct 轨迹的效果最好 。这是因为 ReAct 教会了模型**如何通过与外部环境（如维基百科）交互来获取知识**，这比单纯让模型死记硬背事实知识（容易产生幻觉）更具有通用性和鲁棒性 。


* 
**潜力巨大：** 实验证明，即使只有少量的（3,000条）训练数据，ReAct 模式也能显著提升模型处理复杂任务的能力，展示了其在实际应用中结合更多人工标注数据进一步进化的潜力 。

  
