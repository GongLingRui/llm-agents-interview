https://www.youtube.com/watch?v=reISMhbZ2XE

### 概述
这场网络研讨会核心围绕AI Agent的可观测性如何赋能Agent评估展开，由Harrison（联合创始人兼CEO）和Vivec（LinkedIn Deep Agents团队）主讲。内容先从概念层解析Agent可观测性与评估的核心逻辑、差异及关联，再通过LinkedIn Deep Agents团队的实操案例，展示如何利用LangSmith等工具落地Agent评估与优化，整体结构为概念讲解+实操演示+预留问答环节。

### 关键知识点
1. AI Agent与传统软件、单轮LLM应用的核心差异
   - 解释：传统软件具有确定性，代码是唯一真相源；单轮LLM应用非确定但仅单步调用、上下文相对固定；Agent通过循环/序列调用LLM，非确定性误差叠加，核心逻辑不再仅存在于代码中，需运行后才能知晓实际行为。
   - 示例：LangChain框架定义的Agent仅包含prompt和工具列表，但自然语言输入的多样性导致其行为不可提前预判；调试Agent从调试代码栈转向调试推理过程。
2. Agent可观测性的核心原语（按复杂度升序）
   - 2.1 Runs（单次执行轮次）
     - 解释：对应单次LLM调用，是可观测性基础单元，包含输入（工具、温度等参数、prompt）、输出（含推理、最终响应、工具调用的多组件消息）。
     - 示例：LangSmith中可查看单个Run的工具、输入prompt和输出内容。
   - 2.2 Traces（追踪）
     - 解释：由多个Runs组成（可嵌套），代表Agent无人工干预的完整执行过程，LangSmith中以瀑布视图展示其序列和嵌套关系。
     - 示例：用户发送消息后，Agent调用工具、执行工具的所有Runs汇总为一个Trace。
   - 2.3 Threads（线程）
     - 解释：将多个人工干预间隔的Traces分组，代表完整的人机对话流程。
     - 示例：聊天机器人中，用户每发送一次消息触发一个Trace，所有Trace组合成一个Thread。
3. Agent评估的维度（按执行流程复杂度）
   - 3.1 单步评估（Single-step evaluation）
     - 解释：测试Agent单次LLM调用的行为，验证孤立场景下的决策是否符合预期，易定义通过/失败标准，执行速度快。
     - 示例：验证用户请求安排会议时，Agent是否首先调用“检查日历”工具；Agent架构变更时该评估易失效，通常在架构稳定后构建。
   - 3.2 全轮次评估（Full turn evaluation）
     - 解释：测试Agent从顶层用户输入到最终输出的完整执行过程，验证工具使用、状态变化等真实行为，更贴近实际场景。
     - 示例：测试编码Agent是否能生成可运行且经过测试的代码；初期常仅用5-10个示例输入人工肉眼判断，难定义量化指标，是用户最先构建的评估类型。
   - 3.3 多轮次评估（Multi-turn evaluation）
     - 解释：测试跨多个对话轮次的Thread，验证Agent的记忆、跟进提问、信息整合能力，最贴近生产环境的真实行为。
     - 示例：测试Agent是否记住前序对话提及的信息；难点是后续用户输入依赖前序Agent响应，需确保每一步对齐，指标设计难度高。
4. Agent评估的执行阶段
   - 4.1 离线评估（Offline eval）
     - 解释：上线前基于数据集运行Agent/LLM并评分，用于捕捉回归问题、基准测试，数据集常来自生产环境的Traces。
     - 示例：从LangSmith中提取生产Trace的失败节点作为测试用例，调整prompt后重新运行验证修复效果。
   - 4.2 在线评估（Online eval）
     - 解释：Agent在生产环境运行完成后执行，基于Trace分析，无真值参考，用于标记生产环境问题。
     - 示例：检测LLM是否连续多次调用同一工具、监控输出长度合理性、检测幻觉（输出是否包含输入上下文外的内容）、识别用户反馈中的错误（如“不对”等表述）。
   - 4.3 临时评估（Ad hoc eval）
     - 解释：针对生产数据的一次性探索性分析，用于挖掘特定维度的洞察。
     - 示例：分析ChatLangChain中用户关于LangGraph的提问占比，通过评估器标记相关Run并聚类分析。
5. 可观测性（Traces）对Agent评估的赋能方式
   - 5.1 手动调试：通过Trace查看失败节点，在LangSmith playground中复现并调整；可调试本地和生产环境的Agent问题。
   - 5.2 支撑离线评估：从生产Trace提取测试数据集，定位失败状态并转化为测试用例。
   - 5.3 支撑在线评估：基于生产Trace检测异常工具调用、效率、质量、失败预警等。
   - 5.4 生成临时洞察：LangSmith的Insights分析功能可聚类Trace、对比维度，回答“用户如何使用Agent”“用户为何受挫”等问题。
6. LinkedIn Deep Agents团队的实操案例
   - 6.1 工具与框架：Deep Agents（自定义Agent harness），支持自定义prompt、工具、钩子函数，适配不同模型（CodeX、Opus、Sonnet、Haiku等）。
   - 6.2 评估基准：Terminal Bench/Terminal Bench 2.0（前沿实验室常用的编码Agent评估基准）。
   - 6.3 优化策略
     - 6.3.1 提示工程：编写详细prompt定义Agent行为准则（如“自主工作直至任务完成”）。
     - 6.3.2 中间件（Middlewares）：确定性钩子函数，如预完成检查清单（Agent声称完成前验证是否满足所有步骤）、验证环节（生成测试、代码检查、遵循编码规范）。
     - 6.3.3 子代理（Sub agents）：组合多模型/多供应商模型，优化特定任务表现。
     - 6.3.4 环境上下文注入：自动将文件系统结构、仓库规范、团队编码准则等注入模型上下文，减少错误。
   - 6.4 大规模Trace分析：使用LangSmith Trace Analyzer（基于lang fetch拉取Trace到本地，通过Markdown定义分析规则），批量分析Terminal Bench的89个任务，归类失败模式、提出修复建议，生成可视化报告（如CodeX 5.2 Extra High的通过率65.2%）。
   - 6.5 模型迭代测试：针对新发布的CodeX 5.3、Opus 46，快速在harness中测试并对比表现，优化用户体验。

### 重要注意点和警示
- Agent的非确定性导致代码无法完全体现核心逻辑，必须通过运行（Trace）获取真实行为，这与传统软件调试逻辑完全不同，开发中需放弃“代码即真相”的固有认知。
- 单步评估易因Agent架构变更失效，不可作为早期评估的核心依赖，需优先构建全轮次评估，避免评估体系过早失效。
- 多轮次评估虽贴近真实场景，但后续用户输入依赖前序响应，若未做好步骤对齐，评估结果会失真，需设计严格的对齐校验规则，否则无法反映Agent真实表现。
- 在线评估无真值参考，需聚焦“可观测的异常信号”（如重复工具调用、幻觉、用户负面反馈），而非追求精准评分，避免评估方向偏差。
- Agent易出现“路径异常”（如反复编辑同一文件），这类行为通常导致任务失败，需通过可观测性识别并通过钩子函数干预，否则会显著降低生产环境表现。
- 基础设施错误（如沙箱超时、部署环境问题）易被误判为Agent逻辑错误，分析Trace时需区分两类问题，避免无效的Agent优化。
- 若未从开发初期就记录Trace（如接入LangSmith），后续大规模自动化评估、失败模式分析将无法开展，可观测性是评估的前提。

### 实用收获和行动项
- 工具选择：使用LangSmith实现Agent的Run/Trace/Thread记录、手动调试、离线/在线/临时评估、Insights分析；使用LinkedIn Deep Agents harness快速适配不同模型并自定义Agent行为。
- 评估落地步骤：
  1. 上线初期先构建全轮次评估，用5-10个示例输入人工肉眼判断Agent表现；
  2. 待架构稳定后补充单步评估，验证关键节点的决策逻辑；
  3. 逐步引入多轮次评估，设计步骤对齐规则；
  4. 接入LangSmith实现Trace自动日志，从生产Trace提取失败用例补充离线评估数据集。
- Agent优化实操：
  1. 编写详细prompt定义Agent行为边界（如“自主工作直至任务完成，不主动请求人工干预”）；
  2. 开发中间件钩子函数，如预完成检查清单、重复工具调用检测、代码验证环节；
  3. 自动注入环境上下文（文件系统、编码规范等），减少Agent的上下文缺失错误；
  4. 组合多模型（如CodeX 5.3 + Opus 46）构建子代理，优化特定任务表现。
- 大规模分析：使用lang fetch拉取LangSmith中的Trace到本地，编写Markdown规则定义分析维度（如Terminal Bench失败模式分类），生成自动化评估报告并团队评审。
- 最佳实践：尽早将Agent上线到小用户群体，收集生产环境的Trace和用户行为，以此构建更贴合实际的评估数据集，避免“闭门造车”式的离线评估。

### 空白或未解决问题
- 多轮次评估中，如何高效设计“步骤对齐规则”以应对后续用户输入对前序响应的依赖，暂无明确、通用的解决方案。
- 在线评估的量化指标设计：无真值场景下，如何平衡“异常信号检测”的精准度和覆盖度，仍需更多实践探索。
- 不同行业/场景下Agent评估的通用指标体系：目前仅能基于特定场景（如编码Agent的Terminal Bench）设计指标，缺乏跨场景的标准化评估框架。
- LangSmith尚未内置“重复工具调用检测”等通用在线评估器，需开发者自行实现，未来是否集成仍未明确。
- 子代理的最优组合策略：如何选择/搭配不同模型（如CodeX、Opus）以最大化特定任务表现，暂无系统化的方法论。

### 完整性检查
此总结无遗漏地处理了文字稿的所有核心部分，包括Agent可观测性的原语定义、评估的维度与阶段、可观测性对评估的赋能、LinkedIn团队的实操案例、工具使用、各类评估的优缺点与示例，以及所有提及的注意事项和未解决问题，覆盖了文字稿100%的关键内容。
