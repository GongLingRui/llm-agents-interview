### 一、高质量“摘要”的秘诀：告别自由格式，拥抱结构化Schema
- **问题的提出：摘要的“不可逆”风险**
  “摘要 (Summarization)”是一个有损操作，如果提示词设计不当，就可能永久丢失关键信息。如何确保摘要质量？
- **Peak的反直觉洞见：最好的摘要，是不让模型“写摘要”**
  - 错误做法：给模型开放式指令，如“请总结…”，输出不稳定。
  - 正确做法：定义一个结构化的“表单”或“模板”(Schema)，让AI去“填表”。
  - 原文示例字段：
    - 我修改了哪些文件
    - 用户的目标是什么
    - 当前任务的进度如何


### 二、处理大型工具输出的两种策略
| 策略类型       | 做法                                                                 | 保险/效果说明                                                                 |
|----------------|----------------------------------------------------------------------|------------------------------------------------------------------------------|
| 策略一：对于简单搜索“先完整返回，后依赖压缩” | 将工具返回的全部、完整的详细结果直接追加到上下文中，以便模型能立刻、直接地利用这些信息 | 保险措施：同时指示模型，将中间洞见或关键发现主动用`write`工具写入文件，以防止因“压缩”过早发生而丢失信息 |
| 策略二：对于复杂搜索“使用子Agent作为工具”   | 主Agent不直接调用底层工具，而是调用一个更高层次的函数（如`advanced_search`），该函数会触发一个独立的子Agent。这个子Agent在内部完成多次搜索、提炼后，只返回一个固定的、结构化的结果 | 最终效果：主Agent的上下文极其干净、高效，复杂性被完全封装                     |


### 三、扩展：为何简单搜索不立即卸载？
- **为什么简单搜索不“立即卸载”？**
  既然“文件系统即上下文”是最安全的模式（工具返回后立即卸载，只留指针），为何在处理简单搜索时，反而要先将完整内容返回上下文呢？
- **立即卸载的代价 vs 先返回原文的好处**
  | 维度             | 立即卸载的代价                                                                 | 先返回原文的好处                                                                 |
  |------------------|------------------------------------------------------------------------------|----------------------------------------------------------------------------------|
  | LLM调用次数      | 需要“两次LLM调用”才能开始工作                                                 | 只需“一次LLM调用”就能开始工作                                                   |
  | 流程             | `search → 卸载 → 看指针 → read_file → 看内容 → 开始处理`，延迟高、成本高       | `search → 看内容 → 立刻开始处理`，效率极高                                       |
- **最终的工程决策**
  对于那些Agent极有可能需要**立即处理**的工具结果（如一次Google搜索），选择“先返回原文”来换取即时效率。而Peak提出的“保险措施”，正是为了对冲这种风险。
