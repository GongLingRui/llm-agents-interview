# AI Evals：从“黑箱”到“工程”
- 系统性评估(Evals)是AI产品的**绝对基石**
- 核心定义:系统性地测量LLM产品的**非确定性**与**主观性**

## Evals 的核心价值
- 建立信任:确保安全、可靠、有用。
- 监控表现:持续跟踪质量,防止效果退化。
- 驱动改进:(核心)提供数据洞察,指导精准优化。

## 为何困难？三大鸿沟
- 理解鸿沟:难懂用户意图的多样性。
- 规约鸿沟:难将意图精确传达给模型。
- 泛化鸿沟:难处理未见过的边缘情况。

## 行业共识 (Consensus)
- “编写评估将成为产品经理的核心技能。它是用AI打造好产品的关键部分。”
  - Kevin Weil (OpenAI CPO)
- “如果有什么一件事是我们可以教给大家的，那就是编写评估可能是最重要的事情。”
  - Mike Krieger (Anthropic CPO)
- “评估正在成为AI初创公司的真正护城河。”
  - Garry Tan (YC CEO)
- “令人惊讶的是，评估往往就是你所需要的全部。”
  - Greg Brockman (OpenAI)
- “...在幕后，全都是冷酷无情的评估 (ruthless evals)。”
  - Anjney Midha (投资人)

### A-M-I 评估生命周期
（来源：慢学AI bilibili）

#### 1. ANALYZE（分析）
**目标**：弥合“理解鸿沟”
- 数据收集：收集 ~100 条真实 Traces（交互日志）。
- 开放编码：用笔记记录“第一个上游失败点”。
- 轴向编码：将笔记聚类，形成“失败模式分类法”。


#### 2. MEASURE（测量）
**目标**：弥合“泛化鸿沟”
- 代码评估：（客观）检查 JSON、SQL、关键词。
- LLM 判官：（主观）检查语气、帮助性（Pass/Fail）。
- 严格校准：用标注数据（Dev/Test Set）验证判官。


#### 3. IMPROVE（改进）
**目标**：对症下药
- 提示词优化：（首选）澄清指令，添加 Few-shot 示例。
- RAG/结构调优：优化检索质量，任务分解。
- 成本控制：使用分级模型，优化 KV 缓存。


### 核心要点总结
（来源：慢学AI bilibili）
- Evals 是AI产品的核心竞争力，应对**非确定性**和**主观性**。
- 核心流程是 **Analyze - Measure - Improve** 的**持续迭代闭环**。
- **Analyze (分析)** 是**起点**，必须通过“开放编码”**定性理解**失败。
- **Measure (测量)** 通过“代码”和“LLM判官”**定量评估**失败频率。
- **Improve (改进)** 优先从高性价比的事情开始，如：**提示词优化 (Prompt)**。

此外，还有一句话：“如果你不愿意**定期手动查看一些数据**…你做评估就是在**浪费时间**。”
