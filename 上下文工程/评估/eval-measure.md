### Measure（测量）阶段
**副标题**：从“定性假设”到“定量指标”

#### 回顾 (Analyze 产出)
通过分析 ~100条 Trace，得到“失败模式排行榜”。
- 假设：
  - 30%：业务术语-日期解析失败
  - 25%：业务术语-实体定义不全


#### 问题 (The Problem)
这只是基于**小样本的假设**！
- 1. 这个 30% 在全部用户中是真的吗？
- 2. 修复后，如何证明比例下降了？


#### 目标 (The Goal)
将“定性失败模式”转化为可自动运行、持续监控的 **“工程指标”**。
- 核心：构建并**验证**自动化评估器 (Evaluators)。


### 两大前置决策

#### 模块一: 确定评估重点
**核心概念**：区分“规约失败”和“泛化失败”
- **规约失败 (Specification Failure)**
  你的 Prompt 指令不清或不完整。
  案例：AI 忘记 `LIMIT 10`，因为你从没在 Prompt 里要求过。
- **泛化失败 (Generalization Failure)**
  你的 Prompt 指令清晰，但 AI 依然犯错。
  案例：AI 把“上个月”理解错，尽管你定义过它。
- **黄金法则**：“先修复规约失败！”


#### 模块二: 选择评估工具
我们只为“泛化失败”构建评估器。
- **工具A: 代码评估器**
  - 何时用：客观、规则明确的检查。
  - 案例：`SQL语法正确?`、`JSON有效?`、`含关键词?`
  - 优点：快、便宜、100%可靠。
- **工具B: LLM 判官**
  - 何时用：主观、需语义理解的检查。
  - 案例：`日期解析对吗?`、`语气专业吗?`、`摘要忠实吗?`
- **本期焦点**：构建并校准 LLM 判官

### 模块三: 实战 - 构建LLM判官 (1/2)

#### 步骤 1: 编写“评估标准”(Prompt)
一个好的判官 Prompt 必须遵守5大原则：
1. 单一任务: 一个判官只评一件事
2. 二元输出 (Pass/Fail): 不用1-5分
3. 清晰定义: Pass/Fail 的具体标准
4. Few-shot 示例: 清晰的通过/失败案例
5. 结构化输出: 强制 JSON {"reasoning", "answer"}

**BI机器人 Prompt 示例**：

"你是一个QA。评估‘日期解析’是否正确。如果‘上个月’被解析为‘过去30天’，则为‘Fail’；如果解析为‘上一个自然月’，则为‘Pass’。... [few-shot示例] ... 请返回JSON..."


#### 步骤 2: 严格划分数据
**最关键一步！** 必须严格划分针对该失败模式的人工标注数据（e.g., 100-200条）：
- **训练集 (Train Set) (~20%)**：
  目的：挑选 Pass/Fail 案例，放入 Prompt 中作为 Few-shot 示例。
- **开发集 (Dev Set) (~40%)**：
  目的：在此数据集上反复运行判官，对比人工标签，迭代优化 Prompt。
- **测试集 (Test Set) (~40%)**：
  目的：(锁住!) 仅在 Prompt 最终确定后，运行一次，获取判官最终的、无偏的 TPR/TNR。

**核心陷阱**：“数据泄露 (Data Leakage)”

### 模块三: 实战 - 构建LLM判官 (2/2)

#### 步骤 3: 校准循环 (Dev Set)
在**开发集(Dev Set)**上“考验”判官：
1. 运行判官 (得 Pass/Fail)
2. 对比人工标注 (生成**混淆矩阵**)
3. 分析错误 (FN/FP)
4. 迭代优化 Prompt (重复1-3)

- **分析错误**：
  | 错误类型 | 含义 | 行动 |
  | --- | --- | --- |
  | FN (漏报) <br> (判官Pass, 人工Fail) | 判官太松 | 加严规则 |
  | FP (误报) <br> (判官Fail, 人工Pass) | 判官太严 | 修改模糊措辞 |

- **指标（目标：>90%）**：
  | 指标 | 含义 | 通俗解释 |
  | --- | --- | --- |
  | TPR (真阳性率) | 在所有“真·好案例(Pass)”中, 判官成功识别了多少 | 判官“找得全” |
  | TNR (真阴性率) | 在所有“真·坏案例(Fail)”中, 判官成功排除了多少 | 判官“排得准” |


#### 步骤 4: (进阶) 指标修正
**问题**：判官非100%完美 (如 95% TPR)。用它跑1万条数据，得到的“原始通过率 (p_obs)”是有偏差的。

**解决方案 (统计修正)**：

1. 在 **Test Set** 上获取最终 TPR (e.g., 0.90) / TNR (e.g., 0.85)。

2. 在 **1万条未标注数据** 上运行判官, 得 `p_obs` (e.g., 88%)。

3. (重点) 修正偏差：

   原始 `p_obs` (88%) 是虚高的，因为判官有 15% (1–TNR) 的概率把 Fail 错判为 Pass。我们用公式修正这个偏差，得到更接近真实成功率 (θ^) 的估计值。

4. (可选) 用 Bootstrap 计算 **置信区间 (CI)**。

**产出**：真正可信赖的工程指标

### 模块四: 核心陷阱 & 下一步

#### 判官构建的核心陷阱
- **数据泄露 (Data Leakage)**：把 Dev/Test Set 数据用作 Few-shot 示例。
- **标准过宽 (Overly Broad Criteria)**：试图让一个判官同时评估“语气”、“日期”和“格式”（必须拆分！）。
- **跳过验证 (Skipping Validation)**：假设判官“开箱即用”，不做 Dev Set 校准。

#### 判官维护 (Judge Drift)
- **为什么“漂移”？**
  1. 底层模型更新（e.g., `gpt-4o` 升级）
  2. 你的（团队）标准演进了
- **怎么办？**
  1. Pin 住（固定）你的判官模型版本！
  2. 定期（如每月）重新校准！


#### 本期总结 & 下一期预告
- **本期产出**：
  1. 一套自动化的、可信赖的 **评估工具集**。
  2. 对失败模式的 **量化认知**（不再是假设）。
- **下一期 (Improve)**：
  有了工具和目标…
  - 如何 **精准优化 Prompt / RAG**？
  - 如何 **用数据证明** 改进有效？
  - 如何形成 **持续迭代的闭环**？

（如果你觉得本期内容有帮助…请务必一键三连！）
